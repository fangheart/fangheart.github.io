<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="机器学习,有监督学习,无监督学习,线性分类器,SVM支持向量机,logistic回归,随机梯度,贝叶斯模型,K邻近,决策树,集成模型,随机森林,极端随机森林,梯度提升决策树,数据聚类（K-means）," />








  <link rel="shortcut icon" type="image/x-icon" href="/personnal/favicon.ico?v=5.1.0" />






<meta name="description" content="基本知识1.1.1 机器学习的任务机器学习的任务种类较多，但是常规来讲可以以监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）。其中监督学习关注对事物未知表现的预测，一般包括分类问题（classification）、回归问题（Regression）。无监督学习则倾向于对事物本身特性的分析，常用的技术包括数据降维（Dimensionality">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习基础-有监督模型（分类、回归）、无监督模型（聚合、降维）">
<meta property="og:url" content="http://www.fangheart.top/2017/06/11/机器学习基础-有监督模型（分类、回归）、无监督模型（聚合、降维）/index.html">
<meta property="og:site_name" content="FangHeart's blog">
<meta property="og:description" content="基本知识1.1.1 机器学习的任务机器学习的任务种类较多，但是常规来讲可以以监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）。其中监督学习关注对事物未知表现的预测，一般包括分类问题（classification）、回归问题（Regression）。无监督学习则倾向于对事物本身特性的分析，常用的技术包括数据降维（Dimensionality">
<meta property="og:image" content="http://ocef6bnjz.bkt.clouddn.com/17-6-11/34873578.jpg">
<meta property="og:image" content="http://ocef6bnjz.bkt.clouddn.com/17-6-11/75506981.jpg">
<meta property="og:image" content="http://ocef6bnjz.bkt.clouddn.com/17-6-11/17242026.jpg">
<meta property="og:image" content="http://ocef6bnjz.bkt.clouddn.com/17-6-11/26283578.jpg">
<meta property="og:image" content="http://ocef6bnjz.bkt.clouddn.com/17-6-12/89916558.jpg">
<meta property="og:image" content="http://ocef6bnjz.bkt.clouddn.com/17-6-12/20557726.jpg">
<meta property="og:image" content="http://ocef6bnjz.bkt.clouddn.com/17-6-12/60807365.jpg">
<meta property="og:image" content="http://ocef6bnjz.bkt.clouddn.com/17-6-12/79502870.jpg">
<meta property="og:image" content="http://ocef6bnjz.bkt.clouddn.com/17-6-13/91380054.jpg">
<meta property="og:image" content="http://ocef6bnjz.bkt.clouddn.com/17-6-13/59756492.jpg">
<meta property="og:updated_time" content="2017-06-13T02:50:13.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习基础-有监督模型（分类、回归）、无监督模型（聚合、降维）">
<meta name="twitter:description" content="基本知识1.1.1 机器学习的任务机器学习的任务种类较多，但是常规来讲可以以监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）。其中监督学习关注对事物未知表现的预测，一般包括分类问题（classification）、回归问题（Regression）。无监督学习则倾向于对事物本身特性的分析，常用的技术包括数据降维（Dimensionality">
<meta name="twitter:image" content="http://ocef6bnjz.bkt.clouddn.com/17-6-11/34873578.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.fangheart.top/2017/06/11/机器学习基础-有监督模型（分类、回归）、无监督模型（聚合、降维）/"/>





  <title> 机器学习基础-有监督模型（分类、回归）、无监督模型（聚合、降维） | FangHeart's blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">FangHeart's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">爱生活，爱编码。</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocapitalize="off" autocomplete="off" autocorrect="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.fangheart.top/2017/06/11/机器学习基础-有监督模型（分类、回归）、无监督模型（聚合、降维）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="FangHeart">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/personnal/author.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="FangHeart's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                机器学习基础-有监督模型（分类、回归）、无监督模型（聚合、降维）
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-06-11T17:10:15+08:00">
                2017-06-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a class="cloud-tie-join-count" href="/2017/06/11/机器学习基础-有监督模型（分类、回归）、无监督模型（聚合、降维）/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count join-count" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2017/06/11/机器学习基础-有监督模型（分类、回归）、无监督模型（聚合、降维）/" class="leancloud_visitors" data-flag-title="机器学习基础-有监督模型（分类、回归）、无监督模型（聚合、降维）">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="基本知识"><a href="#基本知识" class="headerlink" title="基本知识"></a>基本知识</h2><h3 id="1-1-1-机器学习的任务"><a href="#1-1-1-机器学习的任务" class="headerlink" title="1.1.1 机器学习的任务"></a>1.1.1 机器学习的任务</h3><p>机器学习的任务种类较多，但是常规来讲可以以监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）。其中监督学习关注对事物未知表现的预测，一般包括分类问题（classification）、回归问题（Regression）。无监督学习则倾向于对事物本身特性的分析，常用的技术包括数据降维（Dimensionality Reduction）和聚类问题（Clustering）。</p>
<p>分类问题，便是对其所在类别进行预测，类别是离散的，同时也是预先知道数量的。例如根据一个人的身高、体重和三维判定性别，性别是离散的，男和女，同时预先知道其数量。或者说更经典的例子，根据鸢尾花的花瓣、花萼的长宽等数据，判断其所属的鸢尾花亚种，因为鸢尾花的亚种的种类和数量也满足离散和预先知道这两项特征，因此也是分类预测问题。</p>
<p>回归同样是预测问题，只是预测的目标往往是连续的变量，比如根据房屋的面积、地理位置、建筑年代等对房屋销售价格进行预测，销售价格一定是一个连续的变量。</p>
<p>数据降维度是对事物的特征进行压缩和筛选。如果没有特定领域的知识，无法预先确定采样哪些数据，而现如今采样数据成本较低，但是筛选的成本比较高，比如在图像识别中，如今像素分辨率极大，因而若直接使用这些像素信息那么数据维度非常高，所以很难对数据进行处理。因此这样的数据通常需要进行降维，保存其最有区分度的像素组合，从此便能不影响效果，但是数据维度降低，降低运算难度，也更容易理解。</p>
<p>聚类则是依赖于数据的相似性，把相似的数据样本划分为一个簇，不同于分类问题，我们在大多数情况下不会预先知道簇的数量和每个簇的具体含义。例如购物网站对用户信息和购买习惯进行聚类分析，这样就可以进行分类的广告投放。</p>
<h3 id="1-1-2-机器学习中应用的经验"><a href="#1-1-2-机器学习中应用的经验" class="headerlink" title="1.1.2 机器学习中应用的经验"></a>1.1.2 机器学习中应用的经验</h3><p>习惯性的认为数据就是经验，只有那些对学习任务有用的特定信息才会被列入考虑范围，通常把这些反映数据内在规律的信息叫做特征（Feature）。譬如经典的人脸识别，很少直接用原始图像来进行经验来学习，而是先进行建委，把复杂的数据处理成有助于人脸识别的轮廓特征。</p>
<p>对于监督学习问题，所需要的经验一般包括特征，以及标记/目标(Label/Target)这样才能知道结果是什么，标记/目标的表现形式则取决于监督学习的种类。</p>
<p>无监督学习并不用于做结果预测，那么自然就不需要标记/目标，但是却更加适合对于数据结构的分析。正式因为这个区别可以尝尝获得大量无监督数据，而监督数据的标注因为经常耗费大量的时间、金钱和人力，所以数据量相对较少。</p>
<p>更重要的是，标记/目标的表现形式存在离散、连续变量的区别，从原始数据到特征数据的转换过程中也会遭遇到多种数据类型：类别型特征、数值型特征、甚至是缺失的数据。而学习的过程中需要将这些特征转换为具体的数据参与计算，所以通常要经过缺失数据补全、部分数据过滤、和数据标准化等预先对数据进行处理。</p>
<pre><code>而通常所说的训练集就是既有特征，同时带有目标/标记的数据集。
</code></pre><h3 id="1-1-3-机器学习的性能、精度表现形式"><a href="#1-1-3-机器学习的性能、精度表现形式" class="headerlink" title="1.1.3 机器学习的性能、精度表现形式"></a>1.1.3 机器学习的性能、精度表现形式</h3><p>所谓的性能，便是评价完成任务的质量指标。通常为了评价其完成的质量，我们需要具有相同特征的数据，并将模型的预测结果与相对应的正确结果进行对比。这样的数据成为测试集。（测试集的数据一定不能出现在训练集中，也就是说他们说相互排斥的。）</p>
<p>预测精度问题：</p>
<pre><code>- 对于分类问题来讲，需要根据预测正确类别的百分比来评判其性能，这个指标也通常成为准确性（Accuracy）.
- 对于回归问题需要衡量预测值与实际值之间的偏差大小。
- 而通常这两种预测指标的表现形式有些不足。对于分类问题一般还有召回率（Recall）、精确率（Percision）以及F1指标。
- 对于回归问题一般又有三种关于回归的特有的评价机制如R-squared。【p69】
- 对于聚类问题，如果评估的数据有具体的所属类别，一般采用ARI（Adjust Rand Index）指标，他与分类问题的准确性（Accuracy）类似，但是它也兼顾的考虑到了类簇无法和分类标记意义对应的问题。如果没有具体的所属类别，那么一般使用轮廓系数（Slihouette Coefficient）来度量聚类的结果质量。取值范围【-1，1】数值越大表明效果越好。【P85】
- 对于数据降维如PCA,一般用处理过的数据与未降维的数据来进行对比即可。对比参数可根据数据所属类别来选择。
</code></pre><h2 id="监督学习："><a href="#监督学习：" class="headerlink" title="监督学习："></a>监督学习：</h2><p>机器学习的监督学习模型的任务重点在于，根据已有经验的知识对未知样本的目标/标记进行预测，根据目标预测变量的类别不同，把监督学习任务大体分为分类学习与回归预测两类。<br><img src="http://ocef6bnjz.bkt.clouddn.com/17-6-11/34873578.jpg" alt=""><br>流程即如上体所示，首先需要准备训练数据，可以是文本、图像、音频等，然后抽取所需要的特征，形成特征向量（Feature Vectors），接着把这些特征向量连同对应的标记/目标（即结果）一同送入学习算法（Machine Learning Algorithm）中，训练出一个预测模型(Predictive Model)，然后采用同样的特征抽取方法作用于新的测试数据，得到用于测试的特征向量，最后使用预测模型对这些待测试的特征向量进行预测并得到结果（Expected Label）.</p>
<h3 id="2-1-1分类学习（Classifier）"><a href="#2-1-1分类学习（Classifier）" class="headerlink" title="2.1.1分类学习（Classifier）"></a>2.1.1分类学习（Classifier）</h3><p>分类学习是最为常见的监督学习问题，最为基础的是二分类问题，即是判断是非，从两个类别中选择一个作为预测结果，除此之外还有多类分类即是在多余两个类别中选择一个作为预测结果，甚至还有多标签多分类问题，多标签多分类问题判断一个样本是否同时属于多个不同的类别。</p>
<p>比如实际生活中，遇到许多分类问题，如医生对肿瘤性质进行判定，邮件系统对手写数字进行识别，互联网公司对新闻进行分类，生物学家对物种类型进行鉴定。</p>
<h4 id="2-1-1-1-线性分类器（Linear-Classifier）"><a href="#2-1-1-1-线性分类器（Linear-Classifier）" class="headerlink" title="2.1.1.1 线性分类器（Linear Classifier）"></a>2.1.1.1 线性分类器（Linear Classifier）</h4><p>模型介绍：线性分类器（Linear Classifiers），是一种假设特征与分类结果存在线性关系的模型，这个模型通过累加计算每个维度的特征与各自的权重的乘积来帮助决策。常用的有LogisticRegression与SGDClassifiler。</p>
<p>数据描述：采用UCI的良/恶性乳腺癌肿瘤预测，分别是逻辑斯蒂回归分类，与随机梯度下降分类。</p>
<p>性能分析：准确率（Accuracy）、召回率（Recall）、精确率（Percision）以及F1指标。</p>
<p>特点分析：线性分类器可以说是最为基本和常用的机器学习模型，尽管受限于数据特征与分类目标之间的线性假设，仍然可以在科学研究和工程实践中吧线性分类器作为基准，所使用的模型包括LogisticRegression与SGDClassifiler，相比较，前者对参数的计算采用精确的解析方式，计算时间长但是模型性略高，后者采取随机梯度下降算法估计模型参数，计算时间段，但是模型性能略低。一般而言顺联的数据规模如果超过10W条，考虑到时间的耗用等因素，运用随机梯度算法对模型参数进行估计。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入pandas与numpy工具包。</span></div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># 创建特征列表。</span></div><div class="line">column_names = [<span class="string">'Sample code number'</span>, <span class="string">'Clump Thickness'</span>, <span class="string">'Uniformity of Cell Size'</span>, <span class="string">'Uniformity of Cell Shape'</span>, <span class="string">'Marginal Adhesion'</span>, <span class="string">'Single Epithelial Cell Size'</span>, <span class="string">'Bare Nuclei'</span>, <span class="string">'Bland Chromatin'</span>, <span class="string">'Normal Nucleoli'</span>, <span class="string">'Mitoses'</span>, <span class="string">'Class'</span>]</div><div class="line"></div><div class="line"><span class="comment"># 使用pandas.read_csv函数从互联网读取指定数据。</span></div><div class="line">data = pd.read_csv(<span class="string">'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data'</span>, names = column_names )</div><div class="line"></div><div class="line"><span class="comment"># 将?替换为标准缺失值表示。</span></div><div class="line">data = data.replace(to_replace=<span class="string">'?'</span>, value=np.nan)</div><div class="line"><span class="comment"># 丢弃带有缺失值的数据（只要有一个维度有缺失）。</span></div><div class="line">data = data.dropna(how=<span class="string">'any'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 输出data的数据量和维度。</span></div><div class="line">data.shape</div><div class="line">	</div><div class="line">	(<span class="number">683</span>, <span class="number">11</span>)</div><div class="line"><span class="comment"># 使用sklearn.cross_valiation里的train_test_split模块用于分割数据。</span></div><div class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</div><div class="line"></div><div class="line"><span class="comment"># 随机采样25%的数据用于测试，剩下的75%用于构建训练集合。</span></div><div class="line">X_train, X_test, y_train, y_test = train_test_split(data[column_names[<span class="number">1</span>:<span class="number">10</span>]], data[column_names[<span class="number">10</span>]], test_size=<span class="number">0.25</span>, random_state=<span class="number">33</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 查验训练样本的数量和类别分布。</span></div><div class="line">y_train.value_counts()</div><div class="line"></div><div class="line">	<span class="number">2</span>    <span class="number">344</span></div><div class="line">	<span class="number">4</span>    <span class="number">168</span></div><div class="line">	Name: Class, dtype: int64</div><div class="line"></div><div class="line"><span class="comment"># 查验测试样本的数量和类别分布。</span></div><div class="line">y_test.value_counts()</div><div class="line"></div><div class="line">	</div><div class="line">Out[<span class="number">4</span>]:</div><div class="line"><span class="number">2</span>    <span class="number">100</span></div><div class="line"><span class="number">4</span>     <span class="number">71</span></div><div class="line">Name: Class, dtype: int64</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 从sklearn.preprocessing里导入StandardScaler。</span></div><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</div><div class="line"><span class="comment"># 从sklearn.linear_model里导入LogisticRegression与SGDClassifier。</span></div><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</div><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</div><div class="line"></div><div class="line"><span class="comment"># 标准化数据，保证每个维度的特征数据方差为1，均值为0。使得预测结果不会被某些维度过大的特征值而主导。</span></div><div class="line">ss = StandardScaler()</div><div class="line">X_train = ss.fit_transform(X_train)</div><div class="line">X_test = ss.transform(X_test)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 初始化LogisticRegression与SGDClassifier。</span></div><div class="line">lr = LogisticRegression()</div><div class="line">sgdc = SGDClassifier()</div><div class="line"></div><div class="line"><span class="comment"># 调用LogisticRegression中的fit函数/模块用来训练模型参数。</span></div><div class="line">lr.fit(X_train, y_train)</div><div class="line"><span class="comment"># 使用训练好的模型lr对X_test进行预测，结果储存在变量lr_y_predict中。</span></div><div class="line">lr_y_predict = lr.predict(X_test)</div><div class="line"></div><div class="line"><span class="comment"># 调用SGDClassifier中的fit函数/模块用来训练模型参数。</span></div><div class="line">sgdc.fit(X_train, y_train)</div><div class="line"><span class="comment"># 使用训练好的模型sgdc对X_test进行预测，结果储存在变量sgdc_y_predict中。</span></div><div class="line">sgdc_y_predict = sgdc.predict(X_test)</div><div class="line"></div><div class="line"><span class="comment"># 从sklearn.metrics里导入classification_report模块。</span></div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</div><div class="line"></div><div class="line"><span class="comment"># 使用逻辑斯蒂回归模型自带的评分函数score获得模型在测试集上的准确性结果。</span></div><div class="line"><span class="keyword">print</span> <span class="string">'Accuracy of LR Classifier:'</span>, lr.score(X_test, y_test)</div><div class="line"><span class="comment"># 利用classification_report模块获得LogisticRegression其他三个指标的结果。</span></div><div class="line"><span class="keyword">print</span> classification_report(y_test, lr_y_predict, target_names=[<span class="string">'Benign'</span>, <span class="string">'Malignant'</span>])</div><div class="line">	Accuracy of LR Classifier: <span class="number">0.988304093567</span></div><div class="line">             precision    recall  f1-score   support</div><div class="line"></div><div class="line">     Benign       <span class="number">0.99</span>      <span class="number">0.99</span>      <span class="number">0.99</span>       <span class="number">100</span></div><div class="line">  Malignant       <span class="number">0.99</span>      <span class="number">0.99</span>      <span class="number">0.99</span>        <span class="number">71</span></div><div class="line"></div><div class="line">avg / total       <span class="number">0.99</span>      <span class="number">0.99</span>      <span class="number">0.99</span>       <span class="number">171</span></div><div class="line"></div><div class="line"> <span class="comment"># 使用随机梯度下降模型自带的评分函数score获得模型在测试集上的准确性结果。</span></div><div class="line"><span class="keyword">print</span> <span class="string">'Accuarcy of SGD Classifier:'</span>, sgdc.score(X_test, y_test)</div><div class="line"><span class="comment"># 利用classification_report模块获得SGDClassifier其他三个指标的结果。</span></div><div class="line"><span class="keyword">print</span> classification_report(y_test, sgdc_y_predict, target_names=[<span class="string">'Benign'</span>, <span class="string">'Malignant'</span>])</div><div class="line"></div><div class="line">	Accuarcy of SGD Classifier: <span class="number">0.953216374269</span></div><div class="line">             precision    recall  f1-score   support</div><div class="line"></div><div class="line">     Benign       <span class="number">0.93</span>      <span class="number">0.99</span>      <span class="number">0.96</span>       <span class="number">100</span></div><div class="line">  Malignant       <span class="number">0.98</span>      <span class="number">0.90</span>      <span class="number">0.94</span>        <span class="number">71</span></div><div class="line"></div><div class="line">avg / total       <span class="number">0.95</span>      <span class="number">0.95</span>      <span class="number">0.95</span>       <span class="number">171</span></div><div class="line">``` </div><div class="line"></div><div class="line"><span class="comment">#### 2.1.1.2 支持向量机(Support Vector Classifier) </span></div><div class="line">![](http://ocef6bnjz.bkt.clouddn.com/<span class="number">17</span><span class="number">-6</span><span class="number">-11</span>/<span class="number">50533102.j</span>pg)</div><div class="line">模型介绍： 例如线性分类可能获得多个线性分类线，但是我们更希望选取更好的那一条，即如上图的H3线，可以对更多的数据点有更好的容忍度。所以支持向量机分类器便是根据训练样本的分布，搜索所有可能的线性分类器中最佳的那个。</div><div class="line">而其中可以用来真正帮助决策最优线性分类器模型的数据点叫做“支持向量”，如上图中的虚线连接的两个点。</div><div class="line">数据描述：</div><div class="line">使用支持向量机分类器处理框架Scikit-learn内部集成的手写数字图片集。</div><div class="line">性能分析：准确率（Accuracy）、召回率（Recall）、精确率（Percision）以及F1指标。需要进一步指出的是以上三个参数最先适用于二分类任务，但是在拥有多个类别的项目中（如此例数据的识别有<span class="number">0</span><span class="number">-9</span>共<span class="number">10</span>个数字），因此无法直接计算上述三个指标，通常的做法是注意评估每个类别的三个指标，也就相当于<span class="number">10</span>个二分类任务。而此学习库也的确是这样的做的。</div><div class="line">特点分析：支持向量机模型在机器学习领域繁荣了一段时间，主要原因在于其精妙的模型假设，可以帮助我们在海量甚至更高维度的数据中筛选出对预测任务最为有效的少数训练样本。这样不仅节省了模型学习所需要的数据内存，同时也提高了模型的预测性能。然后要获得此性能，就必须付出更多的计算代价（CPU资源和计算时间）。</div><div class="line">```python</div><div class="line"><span class="comment"># 从sklearn.datasets里导入手写体数字加载器。</span></div><div class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</div><div class="line"><span class="comment"># 从通过数据加载器获得手写体数字的数码图像数据并储存在digits变量中。</span></div><div class="line">digits = load_digits()</div><div class="line"><span class="comment"># 检视数据规模和特征维度。</span></div><div class="line">digits.data.shape</div><div class="line">	</div><div class="line">	(<span class="number">1797</span>, <span class="number">64</span>）</div><div class="line">	</div><div class="line"><span class="comment"># 从sklearn.cross_validation中导入train_test_split用于数据分割。</span></div><div class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</div><div class="line"></div><div class="line"><span class="comment"># 随机选取75%的数据作为训练样本；其余25%的数据作为测试样本。</span></div><div class="line">X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=<span class="number">0.25</span>, random_state=<span class="number">33</span>)</div><div class="line"></div><div class="line">y_train.shape</div><div class="line"></div><div class="line">	(<span class="number">1347</span>,)</div><div class="line">	</div><div class="line">y_test.shape</div><div class="line">	</div><div class="line">	(<span class="number">450</span>,)</div><div class="line">	</div><div class="line"><span class="comment"># 从sklearn.preprocessing里导入数据标准化模块。</span></div><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</div><div class="line"></div><div class="line"><span class="comment"># 从sklearn.svm里导入基于线性假设的支持向量机分类器LinearSVC。</span></div><div class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</div><div class="line"></div><div class="line"><span class="comment"># 从仍然需要对训练和测试的特征数据进行标准化。</span></div><div class="line">ss = StandardScaler()</div><div class="line">X_train = ss.fit_transform(X_train)</div><div class="line">X_test = ss.transform(X_test)</div><div class="line"></div><div class="line"><span class="comment"># 初始化线性假设的支持向量机分类器LinearSVC。</span></div><div class="line">lsvc = LinearSVC()</div><div class="line"><span class="comment">#进行模型训练</span></div><div class="line">lsvc.fit(X_train, y_train)</div><div class="line"><span class="comment"># 利用训练好的模型对测试样本的数字类别进行预测，预测结果储存在变量y_predict中。</span></div><div class="line">y_predict = lsvc.predict(X_test)</div><div class="line"></div><div class="line"><span class="comment"># 使用模型自带的评估函数进行准确性测评。</span></div><div class="line"><span class="keyword">print</span> <span class="string">'The Accuracy of Linear SVC is'</span>, lsvc.score(X_test, y_test)</div><div class="line"></div><div class="line">	The Accuracy of Linear SVC <span class="keyword">is</span> <span class="number">0.953333333333</span></div><div class="line">	</div><div class="line"><span class="comment"># 依然使用sklearn.metrics里面的classification_report模块对预测结果做更加详细的分析。</span></div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</div><div class="line"><span class="keyword">print</span> classification_report(y_test, y_predict, target_names=digits.target_names.astype(str))</div><div class="line"></div><div class="line">	 precision    recall  f1-score   support</div><div class="line"></div><div class="line">          <span class="number">0</span>       <span class="number">0.92</span>      <span class="number">1.00</span>      <span class="number">0.96</span>        <span class="number">35</span></div><div class="line">          <span class="number">1</span>       <span class="number">0.96</span>      <span class="number">0.98</span>      <span class="number">0.97</span>        <span class="number">54</span></div><div class="line">          <span class="number">2</span>       <span class="number">0.98</span>      <span class="number">1.00</span>      <span class="number">0.99</span>        <span class="number">44</span></div><div class="line">          <span class="number">3</span>       <span class="number">0.93</span>      <span class="number">0.93</span>      <span class="number">0.93</span>        <span class="number">46</span></div><div class="line">          <span class="number">4</span>       <span class="number">0.97</span>      <span class="number">1.00</span>      <span class="number">0.99</span>        <span class="number">35</span></div><div class="line">          <span class="number">5</span>       <span class="number">0.94</span>      <span class="number">0.94</span>      <span class="number">0.94</span>        <span class="number">48</span></div><div class="line">          <span class="number">6</span>       <span class="number">0.96</span>      <span class="number">0.98</span>      <span class="number">0.97</span>        <span class="number">51</span></div><div class="line">          <span class="number">7</span>       <span class="number">0.92</span>      <span class="number">1.00</span>      <span class="number">0.96</span>        <span class="number">35</span></div><div class="line">          <span class="number">8</span>       <span class="number">0.98</span>      <span class="number">0.84</span>      <span class="number">0.91</span>        <span class="number">58</span></div><div class="line">          <span class="number">9</span>       <span class="number">0.95</span>      <span class="number">0.91</span>      <span class="number">0.93</span>        <span class="number">44</span></div><div class="line"></div><div class="line">avg / total       <span class="number">0.95</span>      <span class="number">0.95</span>      <span class="number">0.95</span>       <span class="number">450</span></div></pre></td></tr></table></figure></p>
<h4 id="2-1-1-3-朴素贝叶斯-Native-Bayes"><a href="#2-1-1-3-朴素贝叶斯-Native-Bayes" class="headerlink" title="2.1.1.3 朴素贝叶斯(Native Bayes)"></a>2.1.1.3 朴素贝叶斯(Native Bayes)</h4><p>模型介绍：<br>朴素贝叶斯是一个非常简单，但是实用性很强的分类模型，不过和基于线性假设的（线性分类器、支持向量机）不同，朴素贝叶斯分类器的构造基础是贝叶斯理论。<br><img src="http://ocef6bnjz.bkt.clouddn.com/17-6-11/75506981.jpg" alt=""><br>数据描述：<br>朴素贝叶斯模型有着广泛的应用环境，特别是在文本分类的任务中间，包括互联网新闻的分类、垃圾邮件的筛选等。本次实例就采取新闻文本作为实验数据。<br>性能分析：<br>准确率（Accuracy）、召回率（Recall）、精确率（Percision）以及F1指标。通过这个测度对朴素贝叶斯模型在20类新闻文本分类任务上的性能进行评估。<br>特点分析：<br>朴素贝叶斯模型被广泛的运用于海量互联网文本分类任务。由于其较强的特征条件独立假设，使得模型预测所需估计的参数规模从幂指数向线性量级别减少，极大的节约了内存消耗和计算时间。但是也正因为这种强假设的限制，模型训练无法将各个特征之间的联系考量在内，使得该模型在其他数据特征关联性较强的分类任务上的性能表现不佳。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div></pre></td><td class="code"><pre><div class="line"># 从sklearn.datasets里导入新闻数据抓取器fetch_20newsgroups。</div><div class="line">from sklearn.datasets import fetch_20newsgroups</div><div class="line"># 与之前预存的数据不同，fetch_20newsgroups需要即时从互联网下载数据。</div><div class="line">news = fetch_20newsgroups(subset='all')</div><div class="line"># 查验数据规模和细节。</div><div class="line">print len(news.data)</div><div class="line">print news.data[0]</div><div class="line">	</div><div class="line">	WARNING:sklearn.datasets.twenty_newsgroups:Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)</div><div class="line">	18846</div><div class="line">	From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cmu.edu&gt;</div><div class="line">	Subject: Pens fans reactions</div><div class="line">	Organization: Post Office, Carnegie Mellon, Pittsburgh, PA</div><div class="line">	Lines: 12</div><div class="line">	NNTP-Posting-Host: po4.andrew.cmu.edu</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">	I am sure some bashers of Pens fans are pretty confused about the lack</div><div class="line">	of any kind of posts about the recent Pens massacre of the Devils. Actually,</div><div class="line">	I am  bit puzzled too and a bit relieved. However, I am going to put an end</div><div class="line">	to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they</div><div class="line">	are killing those Devils worse than I thought. Jagr just showed you why</div><div class="line">	he is much better than his regular season stats. He is also a lot</div><div class="line">	fo fun to watch in the playoffs. Bowman should let JAgr have a lot of</div><div class="line">	fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final</div><div class="line">	regular season game.          PENS RULE!!!</div><div class="line">	</div><div class="line"># 从sklearn.cross_validation 导入 train_test_split。</div><div class="line">from sklearn.cross_validation import train_test_split</div><div class="line"># 随机采样25%的数据样本作为测试集。</div><div class="line">X_train, X_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.25, random_state=33)</div><div class="line"></div><div class="line"># 从sklearn.feature_extraction.text里导入用于文本特征向量转化模块。详细介绍请读者参考3.1.1.1 特征抽取一节。</div><div class="line">from sklearn.feature_extraction.text import CountVectorizer</div><div class="line"></div><div class="line">vec = CountVectorizer()</div><div class="line">X_train = vec.fit_transform(X_train)</div><div class="line">X_test = vec.transform(X_test)</div><div class="line"></div><div class="line"># 从sklearn.naive_bayes里导入朴素贝叶斯模型。</div><div class="line">from sklearn.naive_bayes import MultinomialNB</div><div class="line"></div><div class="line"># 从使用默认配置初始化朴素贝叶斯模型。</div><div class="line">mnb = MultinomialNB()</div><div class="line"># 利用训练数据对模型参数进行估计。</div><div class="line">mnb.fit(X_train, y_train)</div><div class="line"># 对测试样本进行类别预测，结果存储在变量y_predict中。</div><div class="line">y_predict = mnb.predict(X_test)</div><div class="line"></div><div class="line"># 从sklearn.metrics里导入classification_report用于详细的分类性能报告。</div><div class="line">from sklearn.metrics import classification_report</div><div class="line">print 'The accuracy of Naive Bayes Classifier is', mnb.score(X_test, y_test)</div><div class="line">print classification_report(y_test, y_predict, target_names = news.target_names)</div><div class="line"></div><div class="line">	The accuracy of Naive Bayes Classifier is 0.839770797963</div><div class="line">							  precision    recall  f1-score   support</div><div class="line"></div><div class="line">				 alt.atheism       0.86      0.86      0.86       201</div><div class="line">			   comp.graphics       0.59      0.86      0.70       250</div><div class="line">	 comp.os.ms-windows.misc       0.89      0.10      0.17       248</div><div class="line">	comp.sys.ibm.pc.hardware       0.60      0.88      0.72       240</div><div class="line">	   comp.sys.mac.hardware       0.93      0.78      0.85       242</div><div class="line">			  comp.windows.x       0.82      0.84      0.83       263</div><div class="line">				misc.forsale       0.91      0.70      0.79       257</div><div class="line">				   rec.autos       0.89      0.89      0.89       238</div><div class="line">			 rec.motorcycles       0.98      0.92      0.95       276</div><div class="line">		  rec.sport.baseball       0.98      0.91      0.95       251</div><div class="line">			rec.sport.hockey       0.93      0.99      0.96       233</div><div class="line">				   sci.crypt       0.86      0.98      0.91       238</div><div class="line">			 sci.electronics       0.85      0.88      0.86       249</div><div class="line">					 sci.med       0.92      0.94      0.93       245</div><div class="line">				   sci.space       0.89      0.96      0.92       221</div><div class="line">	  soc.religion.christian       0.78      0.96      0.86       232</div><div class="line">		  talk.politics.guns       0.88      0.96      0.92       251</div><div class="line">	   talk.politics.mideast       0.90      0.98      0.94       231</div><div class="line">		  talk.politics.misc       0.79      0.89      0.84       188</div><div class="line">		  talk.religion.misc       0.93      0.44      0.60       158</div><div class="line"></div><div class="line">				 avg / total       0.86      0.84      0.82      4712</div></pre></td></tr></table></figure>
<h4 id="2-1-1-4-K近邻-K-Nearest-Neighbor"><a href="#2-1-1-4-K近邻-K-Nearest-Neighbor" class="headerlink" title="2.1.1.4 K近邻(K-Nearest Neighbor)"></a>2.1.1.4 K近邻(K-Nearest Neighbor)</h4><p>模型介绍：<br><img src="http://ocef6bnjz.bkt.clouddn.com/17-6-11/17242026.jpg" alt=""><br>假设有一些携带分类标记的训练样本，分布于特征空间中。（有三类颜色蓝绿红，但图黑白将就看吧），不同颜色代表各自的类别，对于一个待分类的样本，选取待分类样本在特征空间中距离最近的K个已标记样本作为参考，来帮助做出分类决策。因此可以得知K值不同得到的分类器可能不同。<br>数据描述：<br>使用K临近算法对生物物种进行分类，并且使用最为著名的鸢尾数据集。<br>性能分析：<br>准确率（Accuracy）、召回率（Recall）、精确率（Percision）以及F1指标。通过这个测度对朴素贝叶斯模型在20类新闻文本分类任务上的性能进行评估。<br>特点分析：<br>K临近分类是非常直观的机器学习模型，K临近算法与其他模型的最大不同在于该模型没有参数训练过程。也就是说没有通过任何学习算法分析训练数据，而只是根据测试样本在训练数据的分布直接作出决策。所以K临近算法属于无参数模型中非常简单的一种。但是这样的决策算法导致了非常高的计算复杂度和内存消耗。因为该模型每处理一个测试样本都需要对所有预先加载在内存的训练样本进行遍历、逐一计算相似度、排序并且选取K个最近邻训练样本的标记，进而做出决策。这是平方级别的算法复杂度，一旦数据量较大，时间消耗会很大。<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div></pre></td><td class="code"><pre><div class="line"># 从sklearn.datasets 导入 iris数据加载器。</div><div class="line">from sklearn.datasets import load_iris</div><div class="line"># 使用加载器读取数据并且存入变量iris。</div><div class="line">iris = load_iris()</div><div class="line"># 查验数据规模。</div><div class="line">iris.data.shape</div><div class="line"></div><div class="line">	(150L, 4L)</div><div class="line"></div><div class="line"># 查看数据说明。对于一名机器学习的实践者来讲，这是一个好习惯。</div><div class="line">print iris.DESCR</div><div class="line">	</div><div class="line">	Iris Plants Database</div><div class="line"></div><div class="line">	Notes</div><div class="line">	-----</div><div class="line">	Data Set Characteristics:</div><div class="line">		:Number of Instances: 150 (50 in each of three classes)</div><div class="line">		:Number of Attributes: 4 numeric, predictive attributes and the class</div><div class="line">		:Attribute Information:</div><div class="line">			- sepal length in cm</div><div class="line">			- sepal width in cm</div><div class="line">			- petal length in cm</div><div class="line">			- petal width in cm</div><div class="line">			- class:</div><div class="line">					- Iris-Setosa</div><div class="line">					- Iris-Versicolour</div><div class="line">					- Iris-Virginica</div><div class="line">		:Summary Statistics:</div><div class="line"></div><div class="line">		============== ==== ==== ======= ===== ====================</div><div class="line">						Min  Max   Mean    SD   Class Correlation</div><div class="line">		============== ==== ==== ======= ===== ====================</div><div class="line">		sepal length:   4.3  7.9   5.84   0.83    0.7826</div><div class="line">		sepal width:    2.0  4.4   3.05   0.43   -0.4194</div><div class="line">		petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)</div><div class="line">		petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)</div><div class="line">		============== ==== ==== ======= ===== ====================</div><div class="line"></div><div class="line">		:Missing Attribute Values: None</div><div class="line">		:Class Distribution: 33.3% for each of 3 classes.</div><div class="line">		:Creator: R.A. Fisher</div><div class="line">		:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)</div><div class="line">		:Date: July, 1988</div><div class="line"></div><div class="line">	This is a copy of UCI ML iris datasets.</div><div class="line">	http://archive.ics.uci.edu/ml/datasets/Iris</div><div class="line"></div><div class="line">	The famous Iris database, first used by Sir R.A Fisher</div><div class="line"></div><div class="line">	This is perhaps the best known database to be found in the</div><div class="line">	pattern recognition literature.  Fisher's paper is a classic in the field and</div><div class="line">	is referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The</div><div class="line">	data set contains 3 classes of 50 instances each, where each class refers to a</div><div class="line">	type of iris plant.  One class is linearly separable from the other 2; the</div><div class="line">	latter are NOT linearly separable from each other.</div><div class="line"></div><div class="line">	References</div><div class="line">	----------</div><div class="line">	   - Fisher,R.A. "The use of multiple measurements in taxonomic problems"</div><div class="line">		 Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to</div><div class="line">		 Mathematical Statistics" (John Wiley, NY, 1950).</div><div class="line">	   - Duda,R.O., &amp; Hart,P.E. (1973) Pattern Classification and Scene Analysis.</div><div class="line">		 (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.</div><div class="line">	   - Dasarathy, B.V. (1980) "Nosing Around the Neighborhood: A New System</div><div class="line">		 Structure and Classification Rule for Recognition in Partially Exposed</div><div class="line">		 Environments".  IEEE Transactions on Pattern Analysis and Machine</div><div class="line">		 Intelligence, Vol. PAMI-2, No. 1, 67-71.</div><div class="line">	   - Gates, G.W. (1972) "The Reduced Nearest Neighbor Rule".  IEEE Transactions</div><div class="line">		 on Information Theory, May 1972, 431-433.</div><div class="line">	   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al"s AUTOCLASS II</div><div class="line">		 conceptual clustering system finds 3 classes in the data.</div><div class="line">	   - Many, many more ...</div><div class="line">	   </div><div class="line"># 从sklearn.cross_validation里选择导入train_test_split用于数据分割。</div><div class="line">from sklearn.cross_validation import train_test_split</div><div class="line"># 从使用train_test_split，利用随机种子random_state采样25%的数据作为测试集。</div><div class="line">X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=33)</div><div class="line"></div><div class="line"># 从sklearn.preprocessing里选择导入数据标准化模块。</div><div class="line">from sklearn.preprocessing import StandardScaler</div><div class="line"># 从sklearn.neighbors里选择导入KNeighborsClassifier，即K近邻分类器。</div><div class="line">from sklearn.neighbors import KNeighborsClassifier</div><div class="line"></div><div class="line"># 对训练和测试的特征数据进行标准化。</div><div class="line">ss = StandardScaler()</div><div class="line">X_train = ss.fit_transform(X_train)</div><div class="line">X_test = ss.transform(X_test)</div><div class="line"></div><div class="line"># 使用K近邻分类器对测试数据进行类别预测，预测结果储存在变量y_predict中。</div><div class="line">knc = KNeighborsClassifier()</div><div class="line">knc.fit(X_train, y_train)</div><div class="line">y_predict = knc.predict(X_test)</div><div class="line"></div><div class="line"># 使用模型自带的评估函数进行准确性测评。</div><div class="line">print 'The accuracy of K-Nearest Neighbor Classifier is', knc.score(X_test, y_test) </div><div class="line"></div><div class="line">	The accuracy of K-Nearest Neighbor Classifier is 0.894736842105</div><div class="line"></div><div class="line"># 依然使用sklearn.metrics里面的classification_report模块对预测结果做更加详细的分析。</div><div class="line">from sklearn.metrics import classification_report</div><div class="line">print classification_report(y_test, y_predict, target_names=iris.target_names)</div><div class="line"></div><div class="line">			  precision    recall  f1-score   support</div><div class="line"></div><div class="line">			 setosa       1.00      1.00      1.00         8</div><div class="line">		 versicolor       0.73      1.00      0.85        11</div><div class="line">		  virginica       1.00      0.79      0.88        19</div><div class="line"></div><div class="line">		avg / total       0.92      0.89      0.90        38</div></pre></td></tr></table></figure></p>
<h4 id="2-1-1-5-决策树-Decision-Tree"><a href="#2-1-1-5-决策树-Decision-Tree" class="headerlink" title="2.1.1.5 决策树(Decision Tree)"></a>2.1.1.5 决策树(Decision Tree)</h4><p><img src="http://ocef6bnjz.bkt.clouddn.com/17-6-11/26283578.jpg" alt=""><br>模型介绍：<br>无论是logistic回归还是支持向量机模型，都在某种程度上要求被学习的数据特征和目标之间遵循着线性假设。但实际中很多情况这是不可能的。如一个人的年龄与患流感而死亡之间不存在线程关系，如果非要描述这种非线性关系，那么使用分段函数最为合理。而在机器学习中，决策树就是描述这种非线性关系的不二之选。<br>多层决策树的时候，模型在学习的时候就需要考虑特征节点的选取顺序，常用的度量方式包括信息熵（information Gain）和基尼不纯性（Gini Impurity）。<br>数据描述：<br>泰坦尼克号数据，预测事故中人生存的可能性。<br>性能分析：<br>准确率（Accuracy）、召回率（Recall）、精确率（Percision）以及F1指标。<br>特点分析：<br>相比于其他模型，决策树在模型描述上有巨大的优势，决策树的推断逻辑非常直观，具有清晰的可解释性，也方便了模型的可视化。这些特性同时保证了在使用决策树模型时，是无需考虑对数据量化甚至标准化的，并且仍然属于有参数模型，需要花费更多的时间在训练上。<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div></pre></td><td class="code"><pre><div class="line"># 导入pandas用于数据分析。</div><div class="line">import pandas as pd</div><div class="line"># 利用pandas的read_csv模块直接从互联网收集泰坦尼克号乘客数据。</div><div class="line">titanic = pd.read_csv('http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt')</div><div class="line"></div><div class="line"># 观察一下前几行数据，可以发现，数据种类各异，数值型、类别型，甚至还有缺失数据。</div><div class="line">titanic.head()</div><div class="line">	row.names	pclass	survived	name										age	embarked	home.dest										room		ticket	boat	sex</div><div class="line">	0	1		1st			1	Allen, Miss Elisabeth Walton					29.0000			Southampton	St Louis, MO						B-5		24160 L221	2		female</div><div class="line">	1	2		1st			0	Allison, Miss Helen Loraine						2.0000			Southampton	Montreal, PQ / Chesterville, ON		C26			NaN		NaN		female</div><div class="line">	2	3		1st			0	Allison, Mr Hudson Joshua Creighton				30.0000			Southampton	Montreal, PQ / Chesterville, ON		C26			NaN		(135)	male</div><div class="line">	3	4		1st			0	Allison, Mrs Hudson J.C. (Bessie Waldo Daniels)	25.0000			Southampton	Montreal, PQ / Chesterville, ON		C26			NaN		NaN		female</div><div class="line">	4	5		1st			1	Allison, Master Hudson Trevor	0.9167	Southampton	Montreal, PQ / Chesterville, ON	C22	NaN	11	male</div><div class="line"></div><div class="line">	</div><div class="line"># 使用pandas，数据都转入pandas独有的dataframe格式（二维数据表格），直接使用info()，查看数据的统计特性。</div><div class="line">titanic.info()</div><div class="line">		</div><div class="line">		&lt;class 'pandas.core.frame.DataFrame'&gt;</div><div class="line">		Int64Index: 1313 entries, 0 to 1312</div><div class="line">		Data columns (total 11 columns):</div><div class="line">		row.names    1313 non-null int64</div><div class="line">		pclass       1313 non-null object</div><div class="line">		survived     1313 non-null int64</div><div class="line">		name         1313 non-null object</div><div class="line">		age          633 non-null float64</div><div class="line">		embarked     821 non-null object</div><div class="line">		home.dest    754 non-null object</div><div class="line">		room         77 non-null object</div><div class="line">		ticket       69 non-null object</div><div class="line">		boat         347 non-null object</div><div class="line">		sex          1313 non-null object</div><div class="line">		dtypes: float64(1), int64(2), object(8)</div><div class="line">		memory usage: 123.1+ KB</div><div class="line">		</div><div class="line"># 机器学习有一个不太被初学者重视，并且耗时，但是十分重要的一环，特征的选择，这个需要基于一些背景知识。根据我们对这场事故的了解，sex, age, pclass这些都很有可能是决定幸免与否的关键因素。</div><div class="line">X = titanic[['pclass', 'age', 'sex']]</div><div class="line">y = titanic['survived']</div><div class="line"></div><div class="line"># 对当前选择的特征进行探查。</div><div class="line">X.info()</div><div class="line"></div><div class="line">	&lt;class 'pandas.core.frame.DataFrame'&gt;</div><div class="line">	Int64Index: 1313 entries, 0 to 1312</div><div class="line">	Data columns (total 3 columns):</div><div class="line">	pclass    1313 non-null object</div><div class="line">	age       633 non-null float64</div><div class="line">	sex       1313 non-null object</div><div class="line">	dtypes: float64(1), object(2)</div><div class="line">	memory usage: 41.0+ KB</div><div class="line">	</div><div class="line"># 借由上面的输出，我们设计如下几个数据处理的任务：</div><div class="line"># 1) age这个数据列，只有633个，需要补完。</div><div class="line"># 2) sex 与 pclass两个数据列的值都是类别型的，需要转化为数值特征，用0/1代替。</div><div class="line"></div><div class="line"># 首先我们补充age里的数据，使用平均数或者中位数都是对模型偏离造成最小影响的策略。</div><div class="line">X['age'].fillna(X['age'].mean(), inplace=True)</div><div class="line"></div><div class="line"></div><div class="line"># 对补完的数据重新探查。</div><div class="line">X.info()</div><div class="line">	</div><div class="line">	&lt;class 'pandas.core.frame.DataFrame'&gt;</div><div class="line">	Int64Index: 1313 entries, 0 to 1312</div><div class="line">	Data columns (total 3 columns):</div><div class="line">	pclass    1313 non-null object</div><div class="line">	age       1313 non-null float64</div><div class="line">	sex       1313 non-null object</div><div class="line">	dtypes: float64(1), object(2)</div><div class="line">	memory usage: 41.0+ KB</div><div class="line">	</div><div class="line"># 由此得知，age特征得到了补完。</div><div class="line"></div><div class="line"># 数据分割。</div><div class="line">from sklearn.cross_validation import train_test_split</div><div class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 33)</div><div class="line"></div><div class="line"># 我们使用scikit-learn.feature_extraction中的特征转换器，详见3.1.1.1特征抽取。</div><div class="line">from sklearn.feature_extraction import DictVectorizer</div><div class="line">vec = DictVectorizer(sparse=False)</div><div class="line"></div><div class="line"># 转换特征后，我们发现凡是类别型的特征都单独剥离出来，独成一列特征，数值型的则保持不变。</div><div class="line">X_train = vec.fit_transform(X_train.to_dict(orient='record'))</div><div class="line"></div><div class="line">	['age', 'pclass=1st', 'pclass=2nd', 'pclass=3rd', 'sex=female', 'sex=male']</div><div class="line">	</div><div class="line"># 同样需要对测试数据的特征进行转换。</div><div class="line">X_test = vec.transform(X_test.to_dict(orient='record'))</div><div class="line"></div><div class="line"># 从sklearn.tree中导入决策树分类器。</div><div class="line">from sklearn.tree import DecisionTreeClassifier</div><div class="line"># 使用默认配置初始化决策树分类器。</div><div class="line">dtc = DecisionTreeClassifier()</div><div class="line"># 使用分割到的训练数据进行模型学习。</div><div class="line">dtc.fit(X_train, y_train)</div><div class="line"># 用训练好的决策树模型对测试特征数据进行预测。</div><div class="line">y_predict = dtc.predict(X_test)</div><div class="line"></div><div class="line"># 从sklearn.metrics导入classification_report。</div><div class="line">from sklearn.metrics import classification_report</div><div class="line"># 输出预测准确性。</div><div class="line">print dtc.score(X_test, y_test)</div><div class="line"># 输出更加详细的分类性能。</div><div class="line">print classification_report(y_predict, y_test, target_names = ['died', 'survived'])</div><div class="line"></div><div class="line">		0.781155015198</div><div class="line">             precision    recall  f1-score   support</div><div class="line"></div><div class="line">			   died       0.91      0.78      0.84       236</div><div class="line">		   survived       0.58      0.80      0.67        93</div><div class="line"></div><div class="line">		avg / total       0.81      0.78      0.79       329</div></pre></td></tr></table></figure></p>
<h4 id="2-1-1-6-集成模型-Ensemble-随机森林：Random-Forest-Classifier，梯度提升决策树：Gradient-Tree-Boosting。"><a href="#2-1-1-6-集成模型-Ensemble-随机森林：Random-Forest-Classifier，梯度提升决策树：Gradient-Tree-Boosting。" class="headerlink" title="2.1.1.6 集成模型(Ensemble):随机森林：Random Forest Classifier，梯度提升决策树：Gradient Tree Boosting。"></a>2.1.1.6 集成模型(Ensemble):随机森林：Random Forest Classifier，梯度提升决策树：Gradient Tree Boosting。</h4><p>模型介绍：<br>集成分类模型便是综合考量多个分类器的预测结果，从而做出决策。这种综合考量大体上分为两种：<br>1.一种是利用相同的训练数据同时搭建多个独立的分类模型，然后通过投票的方式，以少数服从多数的原则做出最终的分类决策。比较具有代表性的模型为随机森林分类器。即是在相同的训练数据上同时搭建多棵决策树，一株标准的决策树根据每维特征对预测结果的影响程度进行排序，进而决定不同特征从上至下构建分裂节点的顺序，但如果都这样，那么每一棵树都类似，那么就丧失了多样性，而随机森林分类器在构建过程中放弃了这一固定的排序算法，转而随机选取特征。【也是工业上对比的基准线】<br>2.另一种是按照一定次序搭建多个分类模型。这些模型之间彼此存在依赖关系。每一个后续模型的加入都需要对现有的集成模型的综合性能有所贡献，进而不断的提升更新后的集成模型的性能，并最终期望借助整合多个分类能力较弱的分类器，搭建出具有更强分类能力的模型。比较有代表性的就是梯度提升决策树。与构建随机森林分类器模型不同，每一棵决策树在生成的过程中对吼尽可能的降低整体集成模型在训练集上的拟合误差。<br>数据描述：<br>仍然用泰坦尼克号数据。<br>性能分析：<br>使用多种用于评价分类任务性能的指标，将单一决策树、随机森林分类器以及随机梯度提升决策树进行以下四种参数的对比。准确率（Accuracy）、召回率（Recall）、精确率（Percision）以及F1指标。<br>特点分析：<br>相比于其他单一的学习模型，集成模型可以整合多种模型，或者多次就一种类型的模型进行建模，由于模型估计参数的过程也同样受到概率的影响，具有一定的不确定性，因此集成模型虽然在训练过程中要耗费更多的时间，但是得到的综合模型往往具有更高的性能和更好的稳定性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入pandas，并且重命名为pd。</span></div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line"><span class="comment"># 通过互联网读取泰坦尼克乘客档案，并存储在变量titanic中。</span></div><div class="line">titanic = pd.read_csv(<span class="string">'http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 人工选取pclass、age以及sex作为判别乘客是否能够生还的特征。</span></div><div class="line">X = titanic[[<span class="string">'pclass'</span>, <span class="string">'age'</span>, <span class="string">'sex'</span>]]</div><div class="line">y = titanic[<span class="string">'survived'</span>]</div><div class="line"></div><div class="line"><span class="comment"># 对于缺失的年龄信息，我们使用全体乘客的平均年龄代替，这样可以在保证顺利训练模型的同时，尽可能不影响预测任务。</span></div><div class="line">X[<span class="string">'age'</span>].fillna(X[<span class="string">'age'</span>].mean(), inplace=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="comment"># 对原始数据进行分割，25%的乘客数据用于测试。</span></div><div class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</div><div class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.25</span>, random_state = <span class="number">33</span>)</div><div class="line"></div><div class="line"><span class="comment"># 对类别型特征进行转化，成为特征向量。</span></div><div class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</div><div class="line"></div><div class="line">vec = DictVectorizer(sparse=<span class="keyword">False</span>)</div><div class="line">X_train = vec.fit_transform(X_train.to_dict(orient=<span class="string">'record'</span>))</div><div class="line">X_test = vec.transform(X_test.to_dict(orient=<span class="string">'record'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 使用单一决策树进行模型训练以及预测分析。</span></div><div class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</div><div class="line"></div><div class="line">dtc = DecisionTreeClassifier()</div><div class="line">dtc.fit(X_train, y_train)</div><div class="line">dtc_y_pred = dtc.predict(X_test</div><div class="line"></div><div class="line"><span class="comment"># 使用随机森林分类器进行集成模型的训练以及预测分析。</span></div><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</div><div class="line"></div><div class="line">rfc = RandomForestClassifier()</div><div class="line">rfc.fit(X_train, y_train)</div><div class="line">rfc_y_pred = rfc.predict(X_test)</div><div class="line"></div><div class="line"><span class="comment"># 使用梯度提升决策树进行集成模型的训练以及预测分析。</span></div><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</div><div class="line"></div><div class="line">gbc = GradientBoostingClassifier()</div><div class="line">gbc.fit(X_train, y_train)</div><div class="line">gbc_y_pred = gbc.predict(X_test)</div><div class="line"></div><div class="line"><span class="comment"># 从sklearn.metrics导入classification_report。</span></div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</div><div class="line"></div><div class="line"><span class="comment"># 输出单一决策树在测试集上的分类准确性，以及更加详细的精确率、召回率、F1指标。</span></div><div class="line"><span class="keyword">print</span> <span class="string">'The accuracy of decision tree is'</span>, dtc.score(X_test, y_test)</div><div class="line"><span class="keyword">print</span> classification_report(dtc_y_pred, y_test)</div><div class="line"></div><div class="line">	The accuracy of decision tree <span class="keyword">is</span> <span class="number">0.781155015198</span></div><div class="line">             precision    recall  f1-score   support</div><div class="line"></div><div class="line">			  <span class="number">0</span>       <span class="number">0.91</span>      <span class="number">0.78</span>      <span class="number">0.84</span>       <span class="number">236</span></div><div class="line">			  <span class="number">1</span>       <span class="number">0.58</span>      <span class="number">0.80</span>      <span class="number">0.67</span>        <span class="number">93</span></div><div class="line"></div><div class="line">	avg / total       <span class="number">0.81</span>      <span class="number">0.78</span>      <span class="number">0.79</span>       <span class="number">329</span></div><div class="line"></div><div class="line"><span class="comment"># 输出随机森林分类器在测试集上的分类准确性，以及更加详细的精确率、召回率、F1指标。</span></div><div class="line"><span class="keyword">print</span> <span class="string">'The accuracy of random forest classifier is'</span>, rfc.score(X_test, y_test)</div><div class="line"><span class="keyword">print</span> classification_report(rfc_y_pred, y_test)</div><div class="line"></div><div class="line">	The accuracy of random forest classifier <span class="keyword">is</span> <span class="number">0.77811550152</span></div><div class="line">             precision    recall  f1-score   support</div><div class="line"></div><div class="line">			  <span class="number">0</span>       <span class="number">0.90</span>      <span class="number">0.77</span>      <span class="number">0.83</span>       <span class="number">235</span></div><div class="line">			  <span class="number">1</span>       <span class="number">0.58</span>      <span class="number">0.79</span>      <span class="number">0.67</span>        <span class="number">94</span></div><div class="line"></div><div class="line">	avg / total       <span class="number">0.81</span>      <span class="number">0.78</span>      <span class="number">0.79</span>       <span class="number">329</span></div><div class="line"></div><div class="line"><span class="comment"># 输出梯度提升决策树在测试集上的分类准确性，以及更加详细的精确率、召回率、F1指标。</span></div><div class="line"><span class="keyword">print</span> <span class="string">'The accuracy of gradient tree boosting is'</span>, gbc.score(X_test, y_test)</div><div class="line"><span class="keyword">print</span> classification_report(gbc_y_pred, y_test)</div><div class="line"></div><div class="line">	The accuracy of gradient tree boosting <span class="keyword">is</span> <span class="number">0.790273556231</span></div><div class="line">             precision    recall  f1-score   support</div><div class="line"></div><div class="line">			  <span class="number">0</span>       <span class="number">0.92</span>      <span class="number">0.78</span>      <span class="number">0.84</span>       <span class="number">239</span></div><div class="line">			  <span class="number">1</span>       <span class="number">0.58</span>      <span class="number">0.82</span>      <span class="number">0.68</span>        <span class="number">90</span></div><div class="line"></div><div class="line">	avg / total       <span class="number">0.83</span>      <span class="number">0.79</span>      <span class="number">0.80</span>       <span class="number">329</span></div></pre></td></tr></table></figure>
<h3 id="2-1-2-回归预测（Regressor）"><a href="#2-1-2-回归预测（Regressor）" class="headerlink" title="2.1.2 回归预测（Regressor）"></a>2.1.2 回归预测（Regressor）</h3><p>回归问题和分类问题的区别在于其预测的目标是连续的变量，比如：价格、降水量等等。比如经典的例子房价预测，以下小节都采用这个例子。</p>
<h4 id="2-1-2-1-线性回归器"><a href="#2-1-2-1-线性回归器" class="headerlink" title="2.1.2.1 线性回归器"></a>2.1.2.1 线性回归器</h4><p>模型介绍：<br>在线性分类器的时候，为了方便将实数域的结果映射到（0,1）区间，引入了逻辑斯蒂函数，而在线性回归问题中，预测目标直接就是实数域上的数值。因此优化目标更为简单，即最小化预测结果与真实值之间的差值。<br><img src="http://ocef6bnjz.bkt.clouddn.com/17-6-12/89916558.jpg" alt=""><br>此处采用线性回归模型LinearRegression和SGDRegressor两种线性模型。<br>数据描述：<br>美国波士顿地区的房价预测。<br>性能分析：<br>不同于类别预测，不能苛求回归预测的数值结果严格的与真实值相同。一般情况下，希望衡量预测值与真实值之间的差距。因此可以通过多种测评函数进行评价，最为直观的评价指标包括平均绝对误差（Mean Absolute Error,MAE）,均方误差（Mean Squared Error,MSE）,这些都是线性模型所优化的额目标。以及最传统的R-squared（用来衡量模型回归结果的波动可被真实值验证的百分比，也暗示了模型在数值回归方面的能力，同时即考量了回归值与真实值之间的差异，同时也反映了真实值的变动）<br><img src="http://ocef6bnjz.bkt.clouddn.com/17-6-12/20557726.jpg" alt=""><br><img src="http://ocef6bnjz.bkt.clouddn.com/17-6-12/60807365.jpg" alt=""><br>特点分析：<br>线性回归其是最为简单的、易用的回归模型。正是因为其对特征与回归目标之间的线性假设，从某种程度上也局限了其应用范围，特别是现实生活中的许多实例数据的各个特征与回归目标之间，绝大数不能保证严格的线性关系。<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div></pre></td><td class="code"><pre><div class="line"># 从sklearn.datasets导入波士顿房价数据读取器。</div><div class="line">from sklearn.datasets import load_boston</div><div class="line"># 从读取房价数据存储在变量boston中。</div><div class="line">boston = load_boston()</div><div class="line"># 输出数据描述。</div><div class="line">print (boston.DESCR)</div><div class="line"></div><div class="line">	Boston House Prices dataset</div><div class="line">		===========================</div><div class="line"></div><div class="line">		Notes</div><div class="line">		------</div><div class="line">		Data Set Characteristics:  </div><div class="line"></div><div class="line">			:Number of Instances: 506 </div><div class="line"></div><div class="line">			:Number of Attributes: 13 numeric/categorical predictive</div><div class="line">			</div><div class="line">			:Median Value (attribute 14) is usually the target</div><div class="line"></div><div class="line">			:Attribute Information (in order):</div><div class="line">				- CRIM     per capita crime rate by town</div><div class="line">				- ZN       proportion of residential land zoned for lots over 25,000 sq.ft.</div><div class="line">				- INDUS    proportion of non-retail business acres per town</div><div class="line">				- CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)</div><div class="line">				- NOX      nitric oxides concentration (parts per 10 million)</div><div class="line">				- RM       average number of rooms per dwelling</div><div class="line">				- AGE      proportion of owner-occupied units built prior to 1940</div><div class="line">				- DIS      weighted distances to five Boston employment centres</div><div class="line">				- RAD      index of accessibility to radial highways</div><div class="line">				- TAX      full-value property-tax rate per $10,000</div><div class="line">				- PTRATIO  pupil-teacher ratio by town</div><div class="line">				- B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town</div><div class="line">				- LSTAT    % lower status of the population</div><div class="line">				- MEDV     Median value of owner-occupied homes in $1000's</div><div class="line"></div><div class="line">			:Missing Attribute Values: None</div><div class="line"></div><div class="line">			:Creator: Harrison, D. and Rubinfeld, D.L.</div><div class="line"></div><div class="line">		This is a copy of UCI ML housing dataset.</div><div class="line">		http://archive.ics.uci.edu/ml/datasets/Housing</div><div class="line"></div><div class="line"></div><div class="line">		This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.</div><div class="line"></div><div class="line">		The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic</div><div class="line">		prices and the demand for clean air', J. Environ. Economics &amp; Management,</div><div class="line">		vol.5, 81-102, 1978.   Used in Belsley, Kuh &amp; Welsch, 'Regression diagnostics</div><div class="line">		...', Wiley, 1980.   N.B. Various transformations are used in the table on</div><div class="line">		pages 244-261 of the latter.</div><div class="line"></div><div class="line">		The Boston house-price data has been used in many machine learning papers that address regression</div><div class="line">		problems.   </div><div class="line">			 </div><div class="line">		**References**</div><div class="line"></div><div class="line">		   - Belsley, Kuh &amp; Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.</div><div class="line">		   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.</div><div class="line">		   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)</div><div class="line"></div><div class="line"># 从sklearn.cross_validation导入数据分割器。</div><div class="line">from sklearn.cross_validation import train_test_split</div><div class="line"></div><div class="line"># 导入numpy并重命名为np。</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">X = boston.data</div><div class="line">y = boston.target</div><div class="line"></div><div class="line"># 随机采样25%的数据构建测试样本，其余作为训练样本。</div><div class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=33, test_size=0.25)</div><div class="line"></div><div class="line"># 分析回归目标值的差异。</div><div class="line">print ("The max target value is", np.max(boston.target))</div><div class="line">print ("The min target value is", np.min(boston.target))</div><div class="line">print ("The average target value is", np.mean(boston.target))</div><div class="line"></div><div class="line">	('The max target value is', 50.0)</div><div class="line">	('The min target value is', 5.0)</div><div class="line">	('The average target value is', 22.532806324110677)</div><div class="line">	</div><div class="line"># 从sklearn.preprocessing导入数据标准化模块。</div><div class="line">from sklearn.preprocessing import StandardScaler</div><div class="line"></div><div class="line"># 分别初始化对特征和目标值的标准化器。</div><div class="line">ss_x = StandardScaler()</div><div class="line">ss_y = StandardScaler()</div><div class="line"># 分别对训练和测试数据的特征以及目标值进行标准化处理。</div><div class="line">X_train = ss_x.fit_transform(X_train)</div><div class="line">X_test = ss_x.transform(X_test)</div><div class="line"></div><div class="line">y_train = ss_y.fit_transform(y_train)</div><div class="line">y_test = ss_y.transform(y_test)</div><div class="line"></div><div class="line"># 从sklearn.linear_model导入LinearRegression。</div><div class="line">from sklearn.linear_model import LinearRegression</div><div class="line"></div><div class="line"># 使用默认配置初始化线性回归器LinearRegression。</div><div class="line">lr = LinearRegression()</div><div class="line"># 使用训练数据进行参数估计。</div><div class="line">lr.fit(X_train, y_train)</div><div class="line"># 对测试数据进行回归预测。</div><div class="line">lr_y_predict = lr.predict(X_test)</div><div class="line"></div><div class="line"># 从sklearn.linear_model导入SGDRegressor。</div><div class="line">from sklearn.linear_model import SGDRegressor</div><div class="line"></div><div class="line"># 使用默认配置初始化线性回归器SGDRegressor。</div><div class="line">sgdr = SGDRegressor()</div><div class="line"># 使用训练数据进行参数估计。</div><div class="line">sgdr.fit(X_train, y_train)</div><div class="line"># 对测试数据进行回归预测。</div><div class="line">sgdr_y_predict = sgdr.predict(X_test)</div><div class="line"></div><div class="line"># 使用LinearRegression模型自带的评估模块，并输出评估结果。</div><div class="line">print 'The value of default measurement of LinearRegression is', lr.score(X_test, y_test)</div><div class="line"></div><div class="line"># 从sklearn.metrics依次导入r2_score、mean_squared_error以及mean_absoluate_error用于回归性能的评估。</div><div class="line">from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error</div><div class="line"></div><div class="line"># 使用r2_score模块，并输出评估结果。</div><div class="line">print 'The value of R-squared of LinearRegression is', r2_score(y_test, lr_y_predict)</div><div class="line"></div><div class="line"># 使用mean_squared_error模块，并输出评估结果。</div><div class="line">print 'The mean squared error of LinearRegression is', mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(lr_y_predict))</div><div class="line"></div><div class="line"># 使用mean_absolute_error模块，并输出评估结果。</div><div class="line">print 'The mean absoluate error of LinearRegression is', mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(lr_y_predict))</div><div class="line"></div><div class="line"></div><div class="line"># 使用SGDRegressor模型自带的评估模块，并输出评估结果。</div><div class="line">print 'The value of default measurement of SGDRegressor is', sgdr.score(X_test, y_test)</div><div class="line"></div><div class="line"># 使用r2_score模块，并输出评估结果。</div><div class="line">print 'The value of R-squared of SGDRegressor is', r2_score(y_test, sgdr_y_predict)</div><div class="line"></div><div class="line"># 使用mean_squared_error模块，并输出评估结果。</div><div class="line">print 'The mean squared error of SGDRegressor is', mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(sgdr_y_predict))</div><div class="line"></div><div class="line"># 使用mean_absolute_error模块，并输出评估结果。</div><div class="line">print 'The mean absoluate error of SGDRegressor is', mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(sgdr_y_predict))</div><div class="line"></div><div class="line">	The value of default measurement of SGDRegressor is 0.659853975749</div><div class="line">	The value of R-squared of SGDRegressor is 0.659853975749</div><div class="line">	The mean squared error of SGDRegressor is 26.3753630607</div><div class="line">	The mean absoluate error of SGDRegressor is 3.55075990424</div></pre></td></tr></table></figure></p>
<h4 id="2-1-2-2-支持向量机"><a href="#2-1-2-2-支持向量机" class="headerlink" title="2.1.2.2 支持向量机"></a>2.1.2.2 支持向量机</h4><p>模型介绍：<br>与分类中的解释一致，只是用法不同，前者用来分类，而这里进行回归预测，具体的数值预测（在回归中可以对SVM配置不同的核函数）。<br>数据描述：<br>同上文支持向量机分类的数据<br>性能分析：<br>就不同的核函数装配下的支持向量机回归模型在测试集上的回归性能做出评估。会发现在不同的核函数下存在着很大的性能差异，在此例中使用了径向量（Radial basis function）核函数对特征进行非线性映射之后，支持向量机展现了最佳的回归性能。<br>特点分析：<br>可以帮助我们在海量甚至更高维度的数据中筛选出对预测任务最为有效的少数训练样本。这样不仅节省了模型学习所需要的数据内存，同时也提高了模型的预测性能。然后要获得此性能，就必须付出更多的计算代价（CPU资源和计算时间）。不同核函数下性能有差异，所以要多尝试几种核函数模型。<br>核函数：<br>核函数就是通过某种函数计算，将原有的特征映射到更高的维度空间，从而尽可能的达到新的高纬度特征线性可分程度。<br><img src="http://ocef6bnjz.bkt.clouddn.com/17-6-12/79502870.jpg" alt=""><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 从sklearn.svm中导入支持向量机（回归）模型。</span></div><div class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR</div><div class="line"></div><div class="line"><span class="comment"># 使用线性核函数配置的支持向量机进行回归训练，并且对测试样本进行预测。</span></div><div class="line">linear_svr = SVR(kernel=<span class="string">'linear'</span>)</div><div class="line">linear_svr.fit(X_train, y_train)</div><div class="line">linear_svr_y_predict = linear_svr.predict(X_test)</div><div class="line"></div><div class="line"><span class="comment"># 使用多项式核函数配置的支持向量机进行回归训练，并且对测试样本进行预测。</span></div><div class="line">poly_svr = SVR(kernel=<span class="string">'poly'</span>)</div><div class="line">poly_svr.fit(X_train, y_train)</div><div class="line">poly_svr_y_predict = poly_svr.predict(X_test)</div><div class="line"></div><div class="line"><span class="comment"># 使用径向基核函数配置的支持向量机进行回归训练，并且对测试样本进行预测。</span></div><div class="line">rbf_svr = SVR(kernel=<span class="string">'rbf'</span>)</div><div class="line">rbf_svr.fit(X_train, y_train)</div><div class="line">rbf_svr_y_predict = rbf_svr.predict(X_test)</div><div class="line"></div><div class="line"><span class="comment"># 使用R-squared、MSE和MAE指标对三种配置的支持向量机（回归）模型在相同测试集上进行性能评估。</span></div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score, mean_absolute_error, mean_squared_error</div><div class="line"><span class="keyword">print</span> <span class="string">'R-squared value of linear SVR is'</span>, linear_svr.score(X_test, y_test)</div><div class="line"><span class="keyword">print</span> <span class="string">'The mean squared error of linear SVR is'</span>, mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(linear_svr_y_predict))</div><div class="line"><span class="keyword">print</span> <span class="string">'The mean absoluate error of linear SVR is'</span>, mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(linear_svr_y_predict))</div><div class="line"></div><div class="line">		R-squared value of linear SVR <span class="keyword">is</span> <span class="number">0.65171709743</span></div><div class="line">		The mean squared error of linear SVR <span class="keyword">is</span> <span class="number">0.31360572651</span></div><div class="line">		The mean absoluate error of linear SVR <span class="keyword">is</span> <span class="number">0.369259810963</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> <span class="string">'R-squared value of Poly SVR is'</span>, poly_svr.score(X_test, y_test)</div><div class="line"><span class="keyword">print</span> <span class="string">'The mean squared error of Poly SVR is'</span>, mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(poly_svr_y_predict))</div><div class="line"><span class="keyword">print</span> <span class="string">'The mean absoluate error of Poly SVR is'</span>, mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(poly_svr_y_predict)</div><div class="line"></div><div class="line">	R-squared value of Poly SVR <span class="keyword">is</span> <span class="number">0.404454058003</span></div><div class="line">	The mean squared error of Poly SVR <span class="keyword">is</span> <span class="number">0.536249745341</span></div><div class="line">	The mean absoluate error of Poly SVR <span class="keyword">is</span> <span class="number">0.404323590015</span></div><div class="line"></div><div class="line"><span class="keyword">print</span> <span class="string">'R-squared value of RBF SVR is'</span>, rbf_svr.score(X_test, y_test)</div><div class="line"><span class="keyword">print</span> <span class="string">'The mean squared error of RBF SVR is'</span>, mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(rbf_svr_y_predict))</div><div class="line"><span class="keyword">print</span> <span class="string">'The mean absoluate error of RBF SVR is'</span>, mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(rbf_svr_y_predict))</div><div class="line"></div><div class="line">	R-squared value of RBF SVR <span class="keyword">is</span> <span class="number">0.756406891227</span></div><div class="line">	The mean squared error of RBF SVR <span class="keyword">is</span> <span class="number">0.21933948892</span></div><div class="line">	The mean absoluate error of RBF SVR <span class="keyword">is</span> <span class="number">0.280992190922</span></div></pre></td></tr></table></figure></p>
<h4 id="2-1-2-3-K近邻"><a href="#2-1-2-3-K近邻" class="headerlink" title="2.1.2.3 K近邻"></a>2.1.2.3 K近邻</h4><p>模型介绍：<br>不需要训练参数，对于待预测的测试样本选取K个最邻近的点进行回归值的决策。（自然也衍生出了待测试样本回归值的不同方式，即到底是对K个临近目标值使用普通的算术平均算法，还是同时考虑距离的差异进行加权平均，一下代码也会配置不同配置的K临近回归模型来比较性能差异）【预测方式为平均回归的K临近回归和加权回归的K临近回归】<br>数据描述：<br>仍然使用波士顿房价数据。<br>性能分析：<br>仍然使用R-squared、MSE、MAE三种指标。发现此例中加权平均的方式回归预测房价更具有好的预测性。<br>特点分析：<br>K临近回归于K临近分类一样，都属于无参数模型，同样没有训练的过程。但是模型的计算方法非常直观。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 从sklearn.neighbors导入KNeighborRegressor（K近邻回归器）。</span></div><div class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor</div><div class="line"></div><div class="line"><span class="comment"># 初始化K近邻回归器，并且调整配置，使得预测的方式为平均回归：weights='uniform'。</span></div><div class="line">uni_knr = KNeighborsRegressor(weights=<span class="string">'uniform'</span>)</div><div class="line">uni_knr.fit(X_train, y_train)</div><div class="line">uni_knr_y_predict = uni_knr.predict(X_test)</div><div class="line"></div><div class="line"><span class="comment"># 初始化K近邻回归器，并且调整配置，使得预测的方式为根据距离加权回归：weights='distance'。</span></div><div class="line">dis_knr = KNeighborsRegressor(weights=<span class="string">'distance'</span>)</div><div class="line">dis_knr.fit(X_train, y_train)</div><div class="line">dis_knr_y_predict = dis_knr.predict(X_test)</div><div class="line"></div><div class="line"><span class="comment"># 使用R-squared、MSE以及MAE三种指标对平均回归配置的K近邻模型在测试集上进行性能评估。</span></div><div class="line"><span class="keyword">print</span> <span class="string">'R-squared value of uniform-weighted KNeighorRegression:'</span>, uni_knr.score(X_test, y_test)</div><div class="line"><span class="keyword">print</span> <span class="string">'The mean squared error of uniform-weighted KNeighorRegression:'</span>, mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(uni_knr_y_predict))</div><div class="line"><span class="keyword">print</span> <span class="string">'The mean absoluate error of uniform-weighted KNeighorRegression'</span>, mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(uni_knr_y_predict))</div><div class="line"></div><div class="line">	R-squared value of uniform-weighted KNeighorRegression: <span class="number">0.690345456461</span></div><div class="line">	The mean squared error of uniform-weighted KNeighorRegression: <span class="number">0.278823443175</span></div><div class="line">	The mean absoluate error of uniform-weighted KNeighorRegression <span class="number">0.319836405678</span></div><div class="line">	</div><div class="line"><span class="comment"># 使用R-squared、MSE以及MAE三种指标对根据距离加权回归配置的K近邻模型在测试集上进行性能评估。</span></div><div class="line"><span class="keyword">print</span> <span class="string">'R-squared value of distance-weighted KNeighorRegression:'</span>, dis_knr.score(X_test, y_test)</div><div class="line"><span class="keyword">print</span> <span class="string">'The mean squared error of distance-weighted KNeighorRegression:'</span>, mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(dis_knr_y_predict))</div><div class="line"><span class="keyword">print</span> <span class="string">'The mean absoluate error of distance-weighted KNeighorRegression:'</span>, mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(dis_knr_y_predict))	</div><div class="line"></div><div class="line">	R-squared value of distance-weighted KNeighorRegression: <span class="number">0.719758997016</span></div><div class="line">	The mean squared error of distance-weighted KNeighorRegression: <span class="number">0.252338494627</span></div><div class="line">	The mean absoluate error of distance-weighted KNeighorRegression: <span class="number">0.302274187769</span></div></pre></td></tr></table></figure></p>
<h4 id="2-1-2-4-回归树"><a href="#2-1-2-4-回归树" class="headerlink" title="2.1.2.4 回归树"></a>2.1.2.4 回归树</h4><p>模型介绍：<br>回归树的叶节点的数据类型不是离散型，而是连续型的。决策树每个叶节点依照训练数据表现的概率倾向决定了其最终的预测类别，而回归树的叶节点却是一个个具体的值，从预测连续这个意义上严格来讲，回归树不能称为回归算法，因为回归树的叶子节点返回的是一团训练数据的均值，而不是具体的连续的预测值。<br>数据描述：<br>仍然采用波士顿房价数据。<br>性能分析：<br>效果显著优于LinearRegression和SGDRegression,所以可知道美国波士顿房价预测问题的特征与目标之间存在一定的非线性关系。<br>特点分析：</p>
<pre><code>- 优点
    - 数模型可以解决非线性特征的问题。
    - 树模型不要求对特征数据进行标准化和统一量化，即数值型和类别型特征都可以直接被应用在树模型的构建和预测过程中
    - 因为上面的优点，树模型也可以直观的输出决策过程，使得预测结果具有可解释性。
- 缺点
    - 因为其可以解决复杂的非线性拟合问题，所以更加容易因为模型搭建过于复杂而丧失对新数据预测的精度（泛化力）
    - 树模型从上至下的预测流程会因为数据细微的更改而发生较大的结构变化，因此预测稳定下较差
    - 依托训练数据构架的最佳树模型是个NP-hard问题，所以有限时间内无法找到最优解，所以经常借助集成模型，在多个次优解中找到更高的模型性能。
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 从sklearn.tree中导入DecisionTreeRegressor。</span></div><div class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</div><div class="line"><span class="comment"># 使用默认配置初始化DecisionTreeRegressor。</span></div><div class="line">dtr = DecisionTreeRegressor()</div><div class="line"><span class="comment"># 用波士顿房价的训练数据构建回归树。</span></div><div class="line">dtr.fit(X_train, y_train)</div><div class="line"><span class="comment"># 使用默认配置的单一回归树对测试数据进行预测，并将预测值存储在变量dtr_y_predict中。</span></div><div class="line">dtr_y_predict = dtr.predict(X_test)</div><div class="line"></div><div class="line"><span class="comment"># 使用R-squared、MSE以及MAE指标对默认配置的回归树在测试集上进行性能评估。</span></div><div class="line"><span class="keyword">print</span> <span class="string">'R-squared value of DecisionTreeRegressor:'</span>, dtr.score(X_test, y_test)</div><div class="line"><span class="keyword">print</span> <span class="string">'The mean squared error of DecisionTreeRegressor:'</span>, mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(dtr_y_predict))</div><div class="line"><span class="keyword">print</span> <span class="string">'The mean absoluate error of DecisionTreeRegressor:'</span>, mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(dtr_y_predict))</div><div class="line"></div><div class="line">	R-squared value of DecisionTreeRegressor: <span class="number">0.692592549712</span></div><div class="line">	The mean squared error of DecisionTreeRegressor: <span class="number">0.276800084273</span></div><div class="line">	The mean absoluate error of DecisionTreeRegressor: <span class="number">0.340675749137</span></div></pre></td></tr></table></figure>
<h4 id="2-1-2-5-集成模型"><a href="#2-1-2-5-集成模型" class="headerlink" title="2.1.2.5 集成模型"></a>2.1.2.5 集成模型</h4><p>模型介绍：<br>同样有随机森林回归和随机梯度提升树模型，此外还加入了随机森林的变种：极限随机森林（Extremely Randomized Tree）,与普通随机森林模型不同的是，极限随机森林在每当构建一棵树的分裂节点（node）的时候，不会任意选取特征，而是先随机收集一部分特征，然后利用信息熵（Information Gain）和基尼不纯性（Gini Impurity）等指标挑选最佳节点特征。<br>数据描述：<br>仍然是波士顿房价数据。<br>性能分析：<br>同样采用R-Squared、MAE、MSE三种预测指标。<br>特点分析：<br>许多业界从事商业分析系统开发和搭建的工作者更加青睐集成模型，并且经常以这些模型的性能表现为基准，与新设计的其他模型性能进行对比。虽然这些集成模型在训练过程中需要耗费更多的时间，但是往往可以提供更高的表现性能和更好的稳定性。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 从sklearn.ensemble中导入RandomForestRegressor、ExtraTreesGressor以及GradientBoostingRegressor。</span></div><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor</div><div class="line"></div><div class="line"><span class="comment"># 使用RandomForestRegressor训练模型，并对测试数据做出预测，结果存储在变量rfr_y_predict中。</span></div><div class="line">rfr = RandomForestRegressor()</div><div class="line">rfr.fit(X_train, y_train)</div><div class="line">rfr_y_predict = rfr.predict(X_test)</div><div class="line"></div><div class="line"><span class="comment"># 使用ExtraTreesRegressor训练模型，并对测试数据做出预测，结果存储在变量etr_y_predict中。</span></div><div class="line">etr = ExtraTreesRegressor()</div><div class="line">etr.fit(X_train, y_train)</div><div class="line">etr_y_predict = etr.predict(X_test)</div><div class="line"></div><div class="line"><span class="comment"># 使用GradientBoostingRegressor训练模型，并对测试数据做出预测，结果存储在变量gbr_y_predict中。</span></div><div class="line">gbr = GradientBoostingRegressor()</div><div class="line">gbr.fit(X_train, y_train)</div><div class="line">gbr_y_predict = gbr.predict(X_test)</div><div class="line"></div><div class="line"><span class="comment"># 使用R-squared、MSE以及MAE指标对默认配置的随机回归森林在测试集上进行性能评估。</span></div><div class="line"><span class="keyword">print</span> <span class="string">'R-squared value of RandomForestRegressor:'</span>, rfr.score(X_test, y_test)</div><div class="line"><span class="keyword">print</span> <span class="string">'The mean squared error of RandomForestRegressor:'</span>, mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(rfr_y_predict))</div><div class="line"><span class="keyword">print</span> <span class="string">'The mean absoluate error of RandomForestRegressor:'</span>, mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(rfr_y_predict))</div><div class="line"></div><div class="line">	R-squared value of RandomForestRegressor: <span class="number">0.840592481412</span></div><div class="line">	The mean squared error of RandomForestRegressor: <span class="number">0.143535931018</span></div><div class="line">	The mean absoluate error of RandomForestRegressor: <span class="number">0.237149352743</span></div><div class="line">	</div><div class="line"><span class="comment"># 使用R-squared、MSE以及MAE指标对默认配置的极端回归森林在测试集上进行性能评估。</span></div><div class="line"><span class="keyword">print</span> <span class="string">'R-squared value of ExtraTreesRegessor:'</span>, etr.score(X_test, y_test)</div><div class="line"><span class="keyword">print</span> <span class="string">'The mean squared error of  ExtraTreesRegessor:'</span>, mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(etr_y_predict))</div><div class="line"><span class="keyword">print</span> <span class="string">'The mean absoluate error of ExtraTreesRegessor:'</span>, mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(etr_y_predict))</div><div class="line"></div><div class="line"><span class="comment"># 利用训练好的极端回归森林模型，输出每种特征对预测目标的贡献度。</span></div><div class="line"><span class="keyword">print</span> np.sort(zip(etr.feature_importances_, boston.feature_names), axis=<span class="number">0</span>)</div><div class="line"></div><div class="line">	R-squared value of ExtraTreesRegessor: <span class="number">0.835582207186</span></div><div class="line">	The mean squared error of  ExtraTreesRegessor: <span class="number">0.148047351696</span></div><div class="line">	The mean absoluate error of ExtraTreesRegessor: <span class="number">0.250649106588</span></div><div class="line">	[[<span class="string">'0.0148831885456'</span> <span class="string">'AGE'</span>]</div><div class="line">	 [<span class="string">'0.0164137658185'</span> <span class="string">'B'</span>]</div><div class="line">	 [<span class="string">'0.0186619396796'</span> <span class="string">'CHAS'</span>]</div><div class="line">	 [<span class="string">'0.0189673119564'</span> <span class="string">'CRIM'</span>]</div><div class="line">	 [<span class="string">'0.0205761600375'</span> <span class="string">'DIS'</span>]</div><div class="line">	 [<span class="string">'0.0255369237188'</span> <span class="string">'INDUS'</span>]</div><div class="line">	 [<span class="string">'0.025890473054'</span> <span class="string">'LSTAT'</span>]</div><div class="line">	 [<span class="string">'0.0295863020532'</span> <span class="string">'NOX'</span>]</div><div class="line">	 [<span class="string">'0.0309240655832'</span> <span class="string">'PTRATIO'</span>]</div><div class="line">	 [<span class="string">'0.0417999802912'</span> <span class="string">'RAD'</span>]</div><div class="line">	 [<span class="string">'0.0463531289155'</span> <span class="string">'RM'</span>]</div><div class="line">	 [<span class="string">'0.314967578338'</span> <span class="string">'TAX'</span>]</div><div class="line">	 [<span class="string">'0.395439182008'</span> <span class="string">'ZN'</span>]]</div><div class="line">	 </div><div class="line"><span class="comment"># 使用R-squared、MSE以及MAE指标对默认配置的梯度提升回归树在测试集上进行性能评估。</span></div><div class="line"><span class="keyword">print</span> <span class="string">'R-squared value of GradientBoostingRegressor:'</span>, gbr.score(X_test, y_test)</div><div class="line"><span class="keyword">print</span> <span class="string">'The mean squared error of GradientBoostingRegressor:'</span>, mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(gbr_y_predict))</div><div class="line"><span class="keyword">print</span> <span class="string">'The mean absoluate error of GradientBoostingRegressor:'</span>, mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(gbr_y_predict))</div><div class="line"></div><div class="line">	R-squared value of GradientBoostingRegressor: <span class="number">0.842602871434</span></div><div class="line">	The mean squared error of GradientBoostingRegressor: <span class="number">12.2047771094</span></div><div class="line">	The mean absoluate error of GradientBoostingRegressor: <span class="number">2.28597618665</span></div></pre></td></tr></table></figure></p>
<h2 id="2-2-无监督学习"><a href="#2-2-无监督学习" class="headerlink" title="2.2 无监督学习"></a>2.2 无监督学习</h2><p>无监督学习着重于发现数据本身的分布特点，与监督学习不同，无监督学习不需要对数据进行标记。这样，在节省大量人工的同时，也让可以利用的数据规模变得不可限量。</p>
<p>从功能角度来说无监督学习模型帮助我们发现数据的“群落”（聚类，分布情况），同样也可以寻找离群的样本，另外对于特征维度非常高的数据样本，我们同样可以通过无监督学习对数据进行降维，但是保留最具有区分性的低纬度特征。这些都是在海量数据处理中非常实用的技术。</p>
<h3 id="2-2-1-数据聚类"><a href="#2-2-1-数据聚类" class="headerlink" title="2.2.1 数据聚类"></a>2.2.1 数据聚类</h3><p>数据聚类是无监督学习的主流应用之一，最为经典并且易用的聚类模型就是K-Means算法。该算法要求我们预先设定聚类的个数，然后不断更新聚类中心，经过几轮这样的迭代后，最终的目标就是让所有数据点到其所属数据类中心距离的平方和趋于稳定。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div></pre></td><td class="code"><pre><div class="line">#### 2.2.1.1 K均值算法(K-means) </div><div class="line">模型介绍： </div><div class="line">K-means的流程可以分为四个阶段。</div><div class="line">	- 1.随机分布K个特徵空间内的点作为初始的聚类中心。</div><div class="line">	- 2.然后，对于根据每个数据的特性向量从K个聚类中心中寻找距离最近的一个，并且把该数据标记为从属于这个聚类中心。</div><div class="line">	- 3.接着，在所有的数据都被标记过聚类中心之后，根据这些数据新分配的类簇，重新对K个聚类中心做计算。</div><div class="line">	- 4.如果一轮下来，所有的数据点从属的聚类中心与上一次的分配的类簇没有变化，那么迭代可以停止，否则回到步骤2继续循环。</div><div class="line"></div><div class="line">过程如下图：</div><div class="line">![](http://ocef6bnjz.bkt.clouddn.com/17-6-12/19461952.jpg)</div><div class="line"></div><div class="line">数据描述：</div><div class="line">手写体数字图像数据的完整版本。</div><div class="line">性能分析：</div><div class="line">1.如果被用来评估的数据本身带有正确的类别信息，就使用Adjusted Rand Index(ARI),这个指标与分类问题中计算准确性的方法类似，同时也兼顾到了类簇无法和分类标记一一对应的问题。</div><div class="line">![](http://ocef6bnjz.bkt.clouddn.com/17-6-12/7029721.jpg)</div><div class="line">特点分析：</div><div class="line">K-means聚类模型所采用的迭代算法，直观易懂并且非常实用，但有2大缺陷：1.容易收敛到局部最优解2.需要预先设定簇的数量。</div><div class="line">![](http://ocef6bnjz.bkt.clouddn.com/17-6-12/15809673.jpg)</div><div class="line">![](http://ocef6bnjz.bkt.clouddn.com/17-6-12/18562748.jpg)</div><div class="line">为了解决以上问题可以参考肘部观察法。</div><div class="line">![](http://ocef6bnjz.bkt.clouddn.com/17-6-12/68747424.jpg)</div><div class="line">从这个拐点对应的K开始，类簇中心的增加不会破坏数据聚类的结构。</div><div class="line">![](http://ocef6bnjz.bkt.clouddn.com/17-6-12/75499726.jpg)</div><div class="line"></div><div class="line">![](http://ocef6bnjz.bkt.clouddn.com/17-6-12/53533178.jpg)</div><div class="line">```python</div><div class="line"># 分别导入numpy、matplotlib以及pandas，用于数学运算、作图以及数据分析。</div><div class="line">import numpy as np</div><div class="line">import matplotlib.pyplot as plt</div><div class="line">import pandas as pd</div><div class="line"></div><div class="line"># 使用pandas分别读取训练数据与测试数据集。</div><div class="line">digits_train = pd.read_csv(&apos;https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra&apos;, header=None)</div><div class="line">digits_test = pd.read_csv(&apos;https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tes&apos;, header=None)</div><div class="line"></div><div class="line"># 从训练与测试数据集上都分离出64维度的像素特征与1维度的数字目标。</div><div class="line">X_train = digits_train[np.arange(64)]</div><div class="line">y_train = digits_train[64]</div><div class="line"></div><div class="line">X_test = digits_test[np.arange(64)]</div><div class="line">y_test = digits_test[64]</div><div class="line"></div><div class="line"># 从sklearn.cluster中导入KMeans模型。</div><div class="line">from sklearn.cluster import KMeans</div><div class="line"></div><div class="line"># 初始化KMeans模型，并设置聚类中心数量为10。</div><div class="line">kmeans = KMeans(n_clusters=10)</div><div class="line">kmeans.fit(X_train)</div><div class="line"></div><div class="line"># 逐条判断每个测试图像所属的聚类中心。</div><div class="line">y_pred = kmeans.predict(X_test)</div><div class="line"></div><div class="line"># 从sklearn导入度量函数库metrics。</div><div class="line">from sklearn import metrics</div><div class="line"># 使用ARI进行KMeans聚类性能评估。</div><div class="line">print metrics.adjusted_rand_score(y_test, y_pred)</div><div class="line"></div><div class="line">	0.671267090907</div><div class="line"></div><div class="line"># 导入numpy。</div><div class="line">import numpy as np</div><div class="line"># 从sklearn.cluster中导入KMeans算法包。</div><div class="line">from sklearn.cluster import KMeans</div><div class="line"># 从sklearn.metrics导入silhouette_score用于计算轮廓系数。</div><div class="line">from sklearn.metrics import silhouette_score</div><div class="line">import matplotlib.pyplot as plt</div><div class="line"></div><div class="line"># 分割出3*2=6个子图，并在1号子图作图。</div><div class="line">plt.subplot(3,2,1)</div><div class="line"></div><div class="line"># 初始化原始数据点。</div><div class="line">x1 = np.array([1, 2, 3, 1, 5, 6, 5, 5, 6, 7, 8, 9, 7, 9])</div><div class="line">x2 = np.array([1, 3, 2, 2, 8, 6, 7, 6, 7, 1, 2, 1, 1, 3])</div><div class="line">X = np.array(zip(x1, x2)).reshape(len(x1), 2)</div><div class="line"></div><div class="line"># 在1号子图做出原始数据点阵的分布。</div><div class="line">plt.xlim([0, 10])</div><div class="line">plt.ylim([0, 10])</div><div class="line">plt.title(&apos;Instances&apos;)</div><div class="line">plt.scatter(x1, x2)</div><div class="line"></div><div class="line">colors = [&apos;b&apos;, &apos;g&apos;, &apos;r&apos;, &apos;c&apos;, &apos;m&apos;, &apos;y&apos;, &apos;k&apos;, &apos;b&apos;]</div><div class="line">markers = [&apos;o&apos;, &apos;s&apos;, &apos;D&apos;, &apos;v&apos;, &apos;^&apos;, &apos;p&apos;, &apos;*&apos;, &apos;+&apos;]</div><div class="line"></div><div class="line">clusters = [2, 3, 4, 5, 8]</div><div class="line">subplot_counter = 1</div><div class="line">sc_scores = []</div><div class="line">for t in clusters:</div><div class="line">    subplot_counter += 1</div><div class="line">    plt.subplot(3, 2, subplot_counter)</div><div class="line">    kmeans_model = KMeans(n_clusters=t).fit(X)</div><div class="line">    for i, l in enumerate(kmeans_model.labels_):</div><div class="line">        plt.plot(x1[i], x2[i], color=colors[l], marker=markers[l], ls=&apos;None&apos;)</div><div class="line">    plt.xlim([0, 10])</div><div class="line">    plt.ylim([0, 10])</div><div class="line">    sc_score = silhouette_score(X, kmeans_model.labels_, metric=&apos;euclidean&apos;)</div><div class="line">    sc_scores.append(sc_score)</div><div class="line"></div><div class="line"># 绘制轮廓系数与不同类簇数量的直观显示图。</div><div class="line">    plt.title(&apos;K = %s, silhouette coefficient= %0.03f&apos; %(t, sc_score))</div><div class="line">    </div><div class="line"># 绘制轮廓系数与不同类簇数量的关系曲线。</div><div class="line">plt.figure()</div><div class="line">plt.plot(clusters, sc_scores, &apos;*-&apos;)</div><div class="line">plt.xlabel(&apos;Number of Clusters&apos;)</div><div class="line">plt.ylabel(&apos;Silhouette Coefficient Score&apos;)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="http://ocef6bnjz.bkt.clouddn.com/17-6-13/91380054.jpg" alt=""><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入必要的工具包。</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</div><div class="line"><span class="keyword">from</span> scipy.spatial.distance <span class="keyword">import</span> cdist</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># 使用均匀分布函数随机三个簇，每个簇周围10个数据样本。</span></div><div class="line">cluster1 = np.random.uniform(<span class="number">0.5</span>, <span class="number">1.5</span>, (<span class="number">2</span>, <span class="number">10</span>))</div><div class="line">cluster2 = np.random.uniform(<span class="number">5.5</span>, <span class="number">6.5</span>, (<span class="number">2</span>, <span class="number">10</span>))</div><div class="line">cluster3 = np.random.uniform(<span class="number">3.0</span>, <span class="number">4.0</span>, (<span class="number">2</span>, <span class="number">10</span>))</div><div class="line"></div><div class="line"><span class="comment"># 绘制30个数据样本的分布图像。</span></div><div class="line">X = np.hstack((cluster1, cluster2, cluster3)).T</div><div class="line"><span class="keyword">print</span> X.shape</div><div class="line"><span class="keyword">print</span> X.shape[<span class="number">0</span>]</div><div class="line">plt.scatter(X[:,<span class="number">0</span>], X[:, <span class="number">1</span>])</div><div class="line">plt.xlabel(<span class="string">'x1'</span>)</div><div class="line">plt.ylabel(<span class="string">'x2'</span>)</div><div class="line">plt.show()</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 测试9种不同聚类中心数量下，每种情况的聚类质量，并作图。</span></div><div class="line">K = range(<span class="number">1</span>, <span class="number">10</span>)</div><div class="line">meandistortions = []</div><div class="line"></div><div class="line"><span class="keyword">for</span> k <span class="keyword">in</span> K:</div><div class="line">    kmeans = KMeans(n_clusters=k)</div><div class="line">    kmeans.fit(X)</div><div class="line">    meandistortions.append(sum(np.min(cdist(X, kmeans.cluster_centers_, <span class="string">'euclidean'</span>), axis=<span class="number">1</span>))/X.shape[<span class="number">0</span>])</div><div class="line">    </div><div class="line">plt.plot(K, meandistortions, <span class="string">'bx-'</span>)</div><div class="line">plt.xlabel(<span class="string">'k'</span>)</div><div class="line">plt.ylabel(<span class="string">'Average Dispersion'</span>)</div><div class="line">plt.title(<span class="string">'Selecting k with the Elbow Method'</span>)</div><div class="line">plt.show()</div><div class="line"></div><div class="line">	[[ <span class="number">0.65437221</span>  <span class="number">0.60824287</span>  <span class="number">1.37405521</span>  <span class="number">1.06883557</span>  <span class="number">0.56351077</span>  <span class="number">0.83423208</span></div><div class="line">	   <span class="number">0.74072001</span>  <span class="number">0.76639986</span>  <span class="number">1.29815858</span>  <span class="number">0.54004198</span>]</div><div class="line">	 [ <span class="number">0.95506086</span>  <span class="number">1.28734408</span>  <span class="number">0.58704345</span>  <span class="number">1.21770559</span>  <span class="number">0.97949615</span>  <span class="number">0.9624782</span></div><div class="line">	   <span class="number">1.39353203</span>  <span class="number">1.13024559</span>  <span class="number">1.1295162</span>   <span class="number">0.86733591</span>]]</div><div class="line">	(<span class="number">30</span>, <span class="number">2</span>)</div><div class="line">	<span class="number">30</span></div></pre></td></tr></table></figure></p>
<p><img src="http://ocef6bnjz.bkt.clouddn.com/17-6-13/59756492.jpg" alt=""></p>
<h4 id="2-2-2-特征降维"><a href="#2-2-2-特征降维" class="headerlink" title="2.2.2 特征降维"></a>2.2.2 特征降维</h4><p>模型介绍：<br>特征降维是无监督学习的另一个应用，其一我们经常在实际项目中遭遇特征维度非常高的训练样本，往往无法借助自己领域的知识人工构建有效特征，其二在数据表现上，我们无法用肉眼观测到超过3个维度的特征。因此特征将为不仅重构了有效的低纬度特征向量，同时也为数据展现提供了可能，在数据降维方法中，主成分分析（Principal Component Analysis）是最为经典和使用的特征降维技术，特别是在辅助图像识别方面，尤为突出的表现。以下示例一PCA压缩为例。</p>
<h4 id="2-2-2-1-主成分分析-Principal-Component-Analysis-PCA"><a href="#2-2-2-1-主成分分析-Principal-Component-Analysis-PCA" class="headerlink" title="2.2.2.1 主成分分析(Principal Component Analysis:PCA)"></a>2.2.2.1 主成分分析(Principal Component Analysis:PCA)</h4><p>数据描述：<br>使用手写字体图像全集数据。<br>性能分析：<br>可以在降维后再用数据进行训练，再用于分类，与未经过降维分类的实验来进行比较，来验证效果的好坏。<br>会发现经过PCA特征亚索和重建后，特征数据会损失2%左右的预测准确性，但是相比于原始数据的六十四维度的特征而言，使用PCA会压缩并且降低68.75%的维度。在于海量数据时候非常有效。<br>特点分析：<br>降维/压缩问题则是选取数据具有代表性的特征，在保持数据多样性的基础上，规避掉大量的特征冗余和噪声，不过这个过程也很有可能会损失一些有用的模式信息，经过大量的时间证明，相较于损失的少部分模型性能，维度压缩能够节省大量用于模型训练的时间，这样一来，使得PCA所带来的模型総合効率变得更为划算。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入numpy工具包。</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="comment"># 初始化一个2*2的线性相关矩阵。</span></div><div class="line">M = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">4</span>]])</div><div class="line"><span class="comment"># 计算2*2线性相关矩阵的秩。</span></div><div class="line">np.linalg.matrix_rank(M, tol=<span class="keyword">None</span>)</div><div class="line"></div><div class="line">	<span class="number">1</span></div><div class="line">	</div><div class="line"><span class="comment"># 导入pandas用于数据读取和处理。</span></div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line"><span class="comment"># 从互联网读入手写体图片识别任务的训练数据，存储在变量digits_train中。</span></div><div class="line">digits_train = pd.read_csv(<span class="string">'https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra'</span>, header=<span class="keyword">None</span>)</div><div class="line"></div><div class="line"><span class="comment"># 从互联网读入手写体图片识别任务的测试数据，存储在变量digits_test中。</span></div><div class="line">digits_test = pd.read_csv(<span class="string">'https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tes'</span>, header=<span class="keyword">None</span>)</div><div class="line"></div><div class="line"><span class="comment"># 分割训练数据的特征向量和标记。</span></div><div class="line">X_digits = digits_train[np.arange(<span class="number">64</span>)]</div><div class="line">y_digits = digits_train[<span class="number">64</span>]</div><div class="line"></div><div class="line"><span class="comment"># 从sklearn.decomposition导入PCA。 </span></div><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line"></div><div class="line"><span class="comment"># 初始化一个可以将高维度特征向量（64维）压缩至2个维度的PCA。 </span></div><div class="line">estimator = PCA(n_components=<span class="number">2</span>)</div><div class="line">X_pca = estimator.fit_transform(X_digits)</div><div class="line"></div><div class="line"><span class="comment"># 显示10类手写体数字图片经PCA压缩后的2维空间分布。 </span></div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_pca_scatter</span><span class="params">()</span>:</span></div><div class="line">    colors = [<span class="string">'black'</span>, <span class="string">'blue'</span>, <span class="string">'purple'</span>, <span class="string">'yellow'</span>, <span class="string">'white'</span>, <span class="string">'red'</span>, <span class="string">'lime'</span>, <span class="string">'cyan'</span>, <span class="string">'orange'</span>, <span class="string">'gray'</span>]</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(len(colors)):</div><div class="line">        px = X_pca[:, <span class="number">0</span>][y_digits.as_matrix() == i]</div><div class="line">        py = X_pca[:, <span class="number">1</span>][y_digits.as_matrix()== i]</div><div class="line">        plt.scatter(px, py, c=colors[i])</div><div class="line">    </div><div class="line">    plt.legend(np.arange(<span class="number">0</span>,<span class="number">10</span>).astype(str))</div><div class="line">    plt.xlabel(<span class="string">'First Principal Component'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Second Principal Component'</span>)</div><div class="line">    plt.show()</div><div class="line">    </div><div class="line">plot_pca_scatter()</div><div class="line"></div><div class="line"><span class="comment"># 对训练数据、测试数据进行特征向量（图片像素）与分类目标的分隔。</span></div><div class="line">X_train = digits_train[np.arange(<span class="number">64</span>)]</div><div class="line">y_train = digits_train[<span class="number">64</span>]</div><div class="line">X_test = digits_test[np.arange(<span class="number">64</span>)]</div><div class="line">y_test = digits_test[<span class="number">64</span>]</div><div class="line"></div><div class="line"><span class="comment"># 导入基于线性核的支持向量机分类器。</span></div><div class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</div><div class="line"></div><div class="line"><span class="comment"># 使用默认配置初始化LinearSVC，对原始64维像素特征的训练数据进行建模，并在测试数据上做出预测，存储在y_predict中。</span></div><div class="line">svc = LinearSVC()</div><div class="line">svc.fit(X_train, y_train)</div><div class="line">y_predict = svc.predict(X_test)</div><div class="line"></div><div class="line"><span class="comment"># 使用PCA将原64维的图像数据压缩到20个维度。</span></div><div class="line">estimator = PCA(n_components=<span class="number">20</span>)</div><div class="line"></div><div class="line"><span class="comment"># 利用训练特征决定（fit）20个正交维度的方向，并转化（transform）原训练特征。</span></div><div class="line">pca_X_train = estimator.fit_transform(X_train)</div><div class="line"><span class="comment"># 测试特征也按照上述的20个正交维度方向进行转化（transform）。</span></div><div class="line">pca_X_test = estimator.transform(X_test)</div><div class="line"></div><div class="line"><span class="comment"># 使用默认配置初始化LinearSVC，对压缩过后的20维特征的训练数据进行建模，并在测试数据上做出预测，存储在pca_y_predict中。</span></div><div class="line">pca_svc = LinearSVC()</div><div class="line">pca_svc.fit(pca_X_train, y_train)</div><div class="line">pca_y_predict = pca_svc.predict(pca_X_test)</div><div class="line"></div><div class="line"><span class="comment"># 从sklearn.metrics导入classification_report用于更加细致的分类性能分析。</span></div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</div><div class="line"></div><div class="line"><span class="comment"># 对使用原始图像高维像素特征训练的支持向量机分类器的性能作出评估。</span></div><div class="line"><span class="keyword">print</span> svc.score(X_test, y_test)</div><div class="line"><span class="keyword">print</span> classification_report(y_test, y_predict, target_names=np.arange(<span class="number">10</span>).astype(str))</div><div class="line"></div><div class="line"><span class="comment"># 对使用PCA压缩重建的低维图像特征训练的支持向量机分类器的性能作出评估。</span></div><div class="line"><span class="keyword">print</span> pca_svc.score(pca_X_test, y_test)</div><div class="line"><span class="keyword">print</span> classification_report(y_test, pca_y_predict, target_names=np.arange(<span class="number">10</span>).astype(str))</div><div class="line"></div><div class="line"></div><div class="line">		<span class="number">0.930996104619</span></div><div class="line">				 precision    recall  f1-score   support</div><div class="line"></div><div class="line">			  <span class="number">0</span>       <span class="number">0.99</span>      <span class="number">0.98</span>      <span class="number">0.99</span>       <span class="number">178</span></div><div class="line">			  <span class="number">1</span>       <span class="number">0.94</span>      <span class="number">0.84</span>      <span class="number">0.89</span>       <span class="number">182</span></div><div class="line">			  <span class="number">2</span>       <span class="number">0.99</span>      <span class="number">0.97</span>      <span class="number">0.98</span>       <span class="number">177</span></div><div class="line">			  <span class="number">3</span>       <span class="number">0.97</span>      <span class="number">0.92</span>      <span class="number">0.94</span>       <span class="number">183</span></div><div class="line">			  <span class="number">4</span>       <span class="number">0.95</span>      <span class="number">0.97</span>      <span class="number">0.96</span>       <span class="number">181</span></div><div class="line">			  <span class="number">5</span>       <span class="number">0.89</span>      <span class="number">0.96</span>      <span class="number">0.93</span>       <span class="number">182</span></div><div class="line">			  <span class="number">6</span>       <span class="number">0.99</span>      <span class="number">0.98</span>      <span class="number">0.99</span>       <span class="number">181</span></div><div class="line">			  <span class="number">7</span>       <span class="number">0.98</span>      <span class="number">0.90</span>      <span class="number">0.94</span>       <span class="number">179</span></div><div class="line">			  <span class="number">8</span>       <span class="number">0.78</span>      <span class="number">0.91</span>      <span class="number">0.84</span>       <span class="number">174</span></div><div class="line">			  <span class="number">9</span>       <span class="number">0.86</span>      <span class="number">0.89</span>      <span class="number">0.87</span>       <span class="number">180</span></div><div class="line"></div><div class="line">	avg / total       <span class="number">0.93</span>      <span class="number">0.93</span>      <span class="number">0.93</span>      <span class="number">1797</span></div><div class="line"></div><div class="line">	<span class="number">0.909293266555</span></div><div class="line">				 precision    recall  f1-score   support</div><div class="line"></div><div class="line">			  <span class="number">0</span>       <span class="number">0.96</span>      <span class="number">0.96</span>      <span class="number">0.96</span>       <span class="number">178</span></div><div class="line">			  <span class="number">1</span>       <span class="number">0.78</span>      <span class="number">0.85</span>      <span class="number">0.82</span>       <span class="number">182</span></div><div class="line">			  <span class="number">2</span>       <span class="number">0.96</span>      <span class="number">0.98</span>      <span class="number">0.97</span>       <span class="number">177</span></div><div class="line">			  <span class="number">3</span>       <span class="number">0.99</span>      <span class="number">0.89</span>      <span class="number">0.94</span>       <span class="number">183</span></div><div class="line">			  <span class="number">4</span>       <span class="number">0.95</span>      <span class="number">0.92</span>      <span class="number">0.93</span>       <span class="number">181</span></div><div class="line">			  <span class="number">5</span>       <span class="number">0.84</span>      <span class="number">0.97</span>      <span class="number">0.90</span>       <span class="number">182</span></div><div class="line">			  <span class="number">6</span>       <span class="number">0.96</span>      <span class="number">0.97</span>      <span class="number">0.96</span>       <span class="number">181</span></div><div class="line">			  <span class="number">7</span>       <span class="number">0.93</span>      <span class="number">0.92</span>      <span class="number">0.93</span>       <span class="number">179</span></div><div class="line">			  <span class="number">8</span>       <span class="number">0.83</span>      <span class="number">0.83</span>      <span class="number">0.83</span>       <span class="number">174</span></div><div class="line">			  <span class="number">9</span>       <span class="number">0.92</span>      <span class="number">0.82</span>      <span class="number">0.86</span>       <span class="number">180</span></div><div class="line"></div><div class="line">	avg / total       <span class="number">0.91</span>      <span class="number">0.91</span>      <span class="number">0.91</span>      <span class="number">1797</span></div></pre></td></tr></table></figure></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="/personnal/wechat-reward-image.jpg" alt="FangHeart WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="/personnal/alipay-reward-image.jpg" alt="FangHeart Alipay"/>
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/有监督学习/" rel="tag"># 有监督学习</a>
          
            <a href="/tags/无监督学习/" rel="tag"># 无监督学习</a>
          
            <a href="/tags/线性分类器/" rel="tag"># 线性分类器</a>
          
            <a href="/tags/SVM支持向量机/" rel="tag"># SVM支持向量机</a>
          
            <a href="/tags/logistic回归/" rel="tag"># logistic回归</a>
          
            <a href="/tags/随机梯度/" rel="tag"># 随机梯度</a>
          
            <a href="/tags/贝叶斯模型/" rel="tag"># 贝叶斯模型</a>
          
            <a href="/tags/K邻近/" rel="tag"># K邻近</a>
          
            <a href="/tags/决策树/" rel="tag"># 决策树</a>
          
            <a href="/tags/集成模型/" rel="tag"># 集成模型</a>
          
            <a href="/tags/随机森林/" rel="tag"># 随机森林</a>
          
            <a href="/tags/极端随机森林/" rel="tag"># 极端随机森林</a>
          
            <a href="/tags/梯度提升决策树/" rel="tag"># 梯度提升决策树</a>
          
            <a href="/tags/数据聚类（K-means）/" rel="tag"># 数据聚类（K-means）</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/05/16/java语言基础-反射、动态代理、JDK新特性/" rel="next" title="java语言基础_反射、动态代理、JDK新特性">
                <i class="fa fa-chevron-left"></i> java语言基础_反射、动态代理、JDK新特性
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/06/13/机器学习模型使用技巧、特征提升、模型正则化-、模型校验、超参数搜索/" rel="prev" title="机器学习模型使用技巧、特征提升、模型正则化 、模型校验、超参数搜索">
                机器学习模型使用技巧、特征提升、模型正则化 、模型校验、超参数搜索 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/personnal/author.jpg"
               alt="FangHeart" />
          <p class="site-author-name" itemprop="name">FangHeart</p>
           
              <p class="site-description motion-element" itemprop="description">不忘初心，方得始终。</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">40</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">38</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/fangheart" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/2013845927/" target="_blank" title="微博">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  微博
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.zhihu.com/people/fangheart" target="_blank" title="知乎">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  知乎
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="mailto:fangheart@fangheart.win" target="_blank" title="邮箱">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  邮箱
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://haroldliu.weebly.com" title="HaroldLiuChi" target="_blank">HaroldLiuChi</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://guangyugao.weebly.com/" title="GaoGuangYu" target="_blank">GaoGuangYu</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://wenjiewang.top" title="WenJieWang" target="_blank">WenJieWang</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://blog.csdn.net/it_dx" title="DuanXiong" target="_blank">DuanXiong</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://dongxicheng.org/recommend/" title="DongXiCheng" target="_blank">DongXiCheng</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#基本知识"><span class="nav-number">1.</span> <span class="nav-text">基本知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-1-机器学习的任务"><span class="nav-number">1.1.</span> <span class="nav-text">1.1.1 机器学习的任务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-2-机器学习中应用的经验"><span class="nav-number">1.2.</span> <span class="nav-text">1.1.2 机器学习中应用的经验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-3-机器学习的性能、精度表现形式"><span class="nav-number">1.3.</span> <span class="nav-text">1.1.3 机器学习的性能、精度表现形式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#监督学习："><span class="nav-number">2.</span> <span class="nav-text">监督学习：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1分类学习（Classifier）"><span class="nav-number">2.1.</span> <span class="nav-text">2.1.1分类学习（Classifier）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-1-1-线性分类器（Linear-Classifier）"><span class="nav-number">2.1.1.</span> <span class="nav-text">2.1.1.1 线性分类器（Linear Classifier）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-1-3-朴素贝叶斯-Native-Bayes"><span class="nav-number">2.1.2.</span> <span class="nav-text">2.1.1.3 朴素贝叶斯(Native Bayes)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-1-4-K近邻-K-Nearest-Neighbor"><span class="nav-number">2.1.3.</span> <span class="nav-text">2.1.1.4 K近邻(K-Nearest Neighbor)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-1-5-决策树-Decision-Tree"><span class="nav-number">2.1.4.</span> <span class="nav-text">2.1.1.5 决策树(Decision Tree)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-1-6-集成模型-Ensemble-随机森林：Random-Forest-Classifier，梯度提升决策树：Gradient-Tree-Boosting。"><span class="nav-number">2.1.5.</span> <span class="nav-text">2.1.1.6 集成模型(Ensemble):随机森林：Random Forest Classifier，梯度提升决策树：Gradient Tree Boosting。</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-2-回归预测（Regressor）"><span class="nav-number">2.2.</span> <span class="nav-text">2.1.2 回归预测（Regressor）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-1-线性回归器"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.1.2.1 线性回归器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-2-支持向量机"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.1.2.2 支持向量机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-3-K近邻"><span class="nav-number">2.2.3.</span> <span class="nav-text">2.1.2.3 K近邻</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-4-回归树"><span class="nav-number">2.2.4.</span> <span class="nav-text">2.1.2.4 回归树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-5-集成模型"><span class="nav-number">2.2.5.</span> <span class="nav-text">2.1.2.5 集成模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-无监督学习"><span class="nav-number">3.</span> <span class="nav-text">2.2 无监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-数据聚类"><span class="nav-number">3.1.</span> <span class="nav-text">2.2.1 数据聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-特征降维"><span class="nav-number">3.1.1.</span> <span class="nav-text">2.2.2 特征降维</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-1-主成分分析-Principal-Component-Analysis-PCA"><span class="nav-number">3.1.2.</span> <span class="nav-text">2.2.2.1 主成分分析(Principal Component Analysis:PCA)</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-FangHeart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">FangHeart</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  

  

  <span id="busuanzi_container_site_uv">
  本站访客数22<span id="busuanzi_value_site_uv"></span>人次
</span>

<span id="busuanzi_container_site_pv">
    本站总访问量52<span id="busuanzi_value_site_pv"></span>次
</span>
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  


  




	





  
    
    <script>
      var cloudTieConfig = {
        url: document.location.href, 
        sourceId: "",
        productKey: "eb7c3e4313a441e99919491234887c0d",
        target: "cloud-tie-wrapper"
      };
    </script>
    <script src="https://img1.ws.126.net/f2e/tie/yun/sdk/loader.js"></script>
  










  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("EFaViLzEeIm1xLj3lRgl6Ktw-gzGzoHsz", "6QrsfTcpIvP6XuQ8mKM70tx9");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
  


  

  <!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/love.js"></script>
<!-- 背景动画 -->
<script type="text/javascript" src="/js/src/particle.js"></script>

</body>
</html>
